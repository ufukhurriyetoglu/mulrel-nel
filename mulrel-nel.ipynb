{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c72ca0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import io\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30361495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "\n",
    "LOWER = False\n",
    "DIGIT_0 = False\n",
    "UNK_TOKEN = \"#UNK#\"\n",
    "\n",
    "BRACKETS = {\"-LCB-\": \"{\", \"-LRB-\": \"(\", \"-LSB-\": \"[\", \"-RCB-\": \"}\", \"-RRB-\": \")\", \"-RSB-\": \"]\"}\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    unk_token = UNK_TOKEN\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2id = {}\n",
    "        self.id2word = []\n",
    "        self.counts = []\n",
    "        self.unk_id = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(token, lower=LOWER, digit_0=DIGIT_0):\n",
    "        if token in [Vocabulary.unk_token, \"<s>\", \"</s>\"]:\n",
    "            return token\n",
    "        elif token in BRACKETS:\n",
    "            token = BRACKETS[token]\n",
    "        else:\n",
    "            if digit_0:\n",
    "                token = re.sub(\"[0-9]\", \"0\", token)\n",
    "\n",
    "        if lower:\n",
    "            return token.lower()\n",
    "        else:\n",
    "            return token\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        voca = Vocabulary()\n",
    "        voca.load_from_file(path)\n",
    "        return voca\n",
    "\n",
    "    def load_from_file(self, path):\n",
    "        self.word2id = {}\n",
    "        self.id2word = []\n",
    "        self.counts = []\n",
    "\n",
    "        f = io.open(path, \"r\", encoding='utf-8', errors='ignore')\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            comps = line.split('\\t')\n",
    "            if len(comps) == 0 or len(comps) > 2: \n",
    "                raise Exception('sthing wrong')\n",
    "\n",
    "            token = Vocabulary.normalize(comps[0].strip())\n",
    "            self.id2word.append(token)\n",
    "            self.word2id[token] = len(self.id2word) - 1\n",
    "\n",
    "            if len(comps) == 2:\n",
    "                self.counts.append(float(comps[1]))\n",
    "            else: \n",
    "                self.counts.append(1)\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        if Vocabulary.unk_token not in self.word2id:\n",
    "            self.id2word.append(Vocabulary.unk_token)\n",
    "            self.word2id[Vocabulary.unk_token] = len(self.id2word) - 1\n",
    "            self.counts.append(1)\n",
    "            \n",
    "        self.unk_id = self.word2id[Vocabulary.unk_token]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.id2word)\n",
    "\n",
    "    def get_id(self, token):\n",
    "        tok = Vocabulary.normalize(token)\n",
    "        return self.word2id.get(tok, self.unk_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72cc1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "############################## removing stopwords #######################\n",
    "\n",
    "STOPWORDS = {'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all',\n",
    "             'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among',\n",
    "             'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone',\n",
    "             'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be',\n",
    "             'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand',\n",
    "             'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'bottom',\n",
    "             'but', 'by', 'call', 'can', 'cannot', 'cant', 'dont', 'co', 'con', 'could', 'couldnt',\n",
    "             'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg',\n",
    "             'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even',\n",
    "             'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen',\n",
    "             'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty',\n",
    "             'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had',\n",
    "             'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein',\n",
    "             'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred',\n",
    "             'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself',\n",
    "             'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may',\n",
    "             'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly',\n",
    "             'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless',\n",
    "             'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now',\n",
    "             'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other',\n",
    "             'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per',\n",
    "             'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming',\n",
    "             'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six',\n",
    "             'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes',\n",
    "             'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their',\n",
    "             'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore',\n",
    "             'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though',\n",
    "             'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward',\n",
    "             'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very',\n",
    "             'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever',\n",
    "             'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether',\n",
    "             'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will',\n",
    "             'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves',\n",
    "             'st', 'years', 'yourselves', 'new', 'used', 'known', 'year', 'later', 'including', 'used',\n",
    "             'end', 'did', 'just', 'best', 'using'}\n",
    "\n",
    "\n",
    "def is_important_word(s):\n",
    "    \"\"\"\n",
    "    an important word is not a stopword, a number, or len == 1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(s) <= 1 or s.lower() in STOPWORDS:\n",
    "            return False\n",
    "        float(s)\n",
    "        return False\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "\n",
    "def is_stopword(s):\n",
    "    return s.lower() in STOPWORDS\n",
    "\n",
    "\n",
    "############################### coloring ###########################\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "\n",
    "def tokgreen(s):\n",
    "    return bcolors.OKGREEN + s + bcolors.ENDC\n",
    "\n",
    "\n",
    "def tfail(s):\n",
    "    return bcolors.FAIL + s + bcolors.ENDC\n",
    "\n",
    "\n",
    "def tokblue(s):\n",
    "    return bcolors.OKBLUE + s + bcolors.ENDC\n",
    "\n",
    "\n",
    "############################ process list of lists ###################\n",
    "\n",
    "def flatten_list_of_lists(list_of_lists):\n",
    "    \"\"\"\n",
    "    making inputs to torch.nn.EmbeddingBag\n",
    "    \"\"\"\n",
    "    list_of_lists = [[]] + list_of_lists\n",
    "    offsets = np.cumsum([len(x) for x in list_of_lists])[:-1]\n",
    "    flatten = sum(list_of_lists[1:], [])\n",
    "    return flatten, offsets\n",
    "\n",
    "\n",
    "def load_voca_embs(voca_path, embs_path):\n",
    "    voca = Vocabulary.load(voca_path)\n",
    "    embs = np.load(embs_path)\n",
    "\n",
    "    # check if sizes are matched\n",
    "    if embs.shape[0] == voca.size() - 1:\n",
    "        unk_emb = np.mean(embs, axis=0, keepdims=True)\n",
    "        embs = np.append(embs, unk_emb, axis=0)\n",
    "    elif embs.shape[0] != voca.size():\n",
    "        print(embs.shape, voca.size())\n",
    "        raise Exception(\"embeddings and vocabulary have differnt number of items \")\n",
    "\n",
    "    return voca, embs\n",
    "\n",
    "\n",
    "def make_equal_len(lists, fill_in=0, to_right=True):\n",
    "    lens = [len(l) for l in lists]\n",
    "    max_len = max(1, max(lens))\n",
    "    if to_right:\n",
    "        eq_lists = [l + [fill_in] * (max_len - len(l)) for l in lists]\n",
    "        mask = [[1.] * l + [0.] * (max_len - l) for l in lens]\n",
    "    else:\n",
    "        eq_lists = [[fill_in] * (max_len - len(l)) + l for l in lists]\n",
    "        mask = [[0.] * (max_len - l) + [1.] * l for l in lens]\n",
    "    return eq_lists, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "311db9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path, model_class, suffix=''):\n",
    "    with io.open(path + '.config', 'r', encoding='utf8') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    word_voca = Vocabulary()\n",
    "    word_voca.__dict__ = config['word_voca']\n",
    "    config['word_voca'] = word_voca\n",
    "    entity_voca = Vocabulary()\n",
    "    entity_voca.__dict__ = config['entity_voca']\n",
    "    config['entity_voca'] = entity_voca\n",
    "\n",
    "    if 'snd_word_voca' in config:\n",
    "        snd_word_voca = Vocabulary()\n",
    "        snd_word_voca.__dict__ = config['snd_word_voca']\n",
    "        config['snd_word_voca'] = snd_word_voca\n",
    "\n",
    "    model = model_class(config)\n",
    "    model.load_state_dict(torch.load(path + '.state_dict' + suffix))\n",
    "    return model\n",
    "\n",
    "\n",
    "class AbstractWordEntity(nn.Module):\n",
    "    \"\"\"\n",
    "    abstract class containing word and entity embeddings and vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super(AbstractWordEntity, self).__init__()\n",
    "        if config is None:\n",
    "            return\n",
    "\n",
    "        self.emb_dims = config['emb_dims']\n",
    "        self.word_voca = config['word_voca']\n",
    "        self.entity_voca = config['entity_voca']\n",
    "        self.freeze_embs = config['freeze_embs']\n",
    "\n",
    "        self.word_embeddings = config['word_embeddings_class'](self.word_voca.size(), self.emb_dims)\n",
    "        self.entity_embeddings = config['entity_embeddings_class'](self.entity_voca.size(), self.emb_dims)\n",
    "\n",
    "        if 'word_embeddings' in config:\n",
    "            self.word_embeddings.weight = nn.Parameter(torch.Tensor(config['word_embeddings']))\n",
    "        if 'entity_embeddings' in config:\n",
    "            self.entity_embeddings.weight = nn.Parameter(torch.Tensor(config['entity_embeddings']))\n",
    "\n",
    "        if 'snd_word_voca' in config:\n",
    "            self.snd_word_voca = config['snd_word_voca']\n",
    "            self.snd_word_embeddings = config['word_embeddings_class'](self.snd_word_voca.size(), self.emb_dims)\n",
    "        if 'snd_word_embeddings' in config:\n",
    "            self.snd_word_embeddings.weight = nn.Parameter(torch.Tensor(config['snd_word_embeddings']))\n",
    "\n",
    "        if self.freeze_embs:\n",
    "            self.word_embeddings.weight.requires_grad = False\n",
    "            self.entity_embeddings.weight.requires_grad = False\n",
    "            if 'snd_word_voca' in config:\n",
    "                self.snd_word_embeddings.weight.requires_grad = False\n",
    "\n",
    "    def print_weight_norm(self):\n",
    "        pass\n",
    "\n",
    "    def save(self, path, suffix='', save_config=True):\n",
    "        torch.save(self.state_dict(), path + '.state_dict' + suffix)\n",
    "\n",
    "        if save_config:\n",
    "            config = {'word_voca': self.word_voca.__dict__,\n",
    "                      'entity_voca': self.entity_voca.__dict__}\n",
    "            if 'snd_word_voca' in self.__dict__:\n",
    "                config['snd_word_voca'] = self.snd_word_voca.__dict__\n",
    "\n",
    "            for k, v in self.__dict__.items():\n",
    "                if not hasattr(v, '__dict__'):\n",
    "                    config[k] = v\n",
    "\n",
    "            with io.open(path + '.config', 'w', encoding='utf8') as f:\n",
    "                json.dump(config, f)\n",
    "\n",
    "    def load_params(self, path, param_names):\n",
    "        params = torch.load(path)\n",
    "        for pname in param_names:\n",
    "            self._parameters[pname].data = params[pname]\n",
    "\n",
    "    def loss(self, scores, grth):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9543050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from nel.local_ctx_att_ranker import LocalCtxAttRanker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class STArgmax(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, scores):\n",
    "        max_values, _ = scores.max(dim=-1, keepdim=True)\n",
    "        return (scores >= max_values).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "class MulRelRanker(LocalCtxAttRanker):\n",
    "    \"\"\"\n",
    "    multi-relational global model with context token attention, using loopy belief propagation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MulRelRanker, self).__init__(config)\n",
    "        self.df = config['df']  # damping factor\n",
    "        self.n_loops = config['n_loops']\n",
    "        self.n_rels = config['n_rels']\n",
    "        self.dr = config['dr']\n",
    "        self.ew_hid_dims = self.emb_dims\n",
    "\n",
    "        self.max_dist = 1000\n",
    "        self.ent_top_n = 1000\n",
    "\n",
    "        self.oracle = config.get('oracle', False)\n",
    "        self.ent_ent_comp = config.get('ent_ent_comp', 'bilinear')  # bilinear, trans_e, fbilinear\n",
    "        self.ctx_comp = config.get('ctx_comp', 'bow')  # bow or rnn\n",
    "\n",
    "        self.mode = config.get('mulrel_type', 'ment-norm')  # ment-norm, rel-norm\n",
    "\n",
    "        # options for ment-norm\n",
    "        self.first_head_uniform = config.get('first_head_uniform', False)\n",
    "        self.use_pad_ent = config.get('use_pad_ent', False)\n",
    "\n",
    "        # options for rel-norm\n",
    "        self.use_stargmax = config.get('use_stargmax', False)\n",
    "\n",
    "        self.use_local = config.get('use_local', False)\n",
    "        self.use_local_only = config.get('use_local_only', False)\n",
    "        self.freeze_local = config.get('freeze_local', False)\n",
    "\n",
    "        if self.freeze_local:\n",
    "            self.att_mat_diag.requires_grad = False\n",
    "            self.tok_score_mat_diag.requires_grad = False\n",
    "\n",
    "        if self.use_local:\n",
    "            self.ent_localctx_comp = torch.nn.Parameter(torch.ones(self.emb_dims))\n",
    "\n",
    "        if self.use_pad_ent:\n",
    "            self.pad_ent_emb = torch.nn.Parameter(torch.randn(1, self.emb_dims) * 0.1)\n",
    "            self.pad_ctx_vec = torch.nn.Parameter(torch.randn(1, self.emb_dims) * 0.1)\n",
    "\n",
    "        self.ctx_layer = torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.emb_dims * 3, self.ew_hid_dims),\n",
    "                torch.nn.Tanh(),\n",
    "                torch.nn.Dropout(p=self.dr))\n",
    "\n",
    "        self.rel_embs = torch.randn(self.n_rels, self.emb_dims) * 0.01\n",
    "        if self.ent_ent_comp == 'bilinear':\n",
    "            self.rel_embs[0] = 1 + torch.randn(self.emb_dims) * 0.01\n",
    "            if self.mode == 'ment-norm' and self.n_rels > 1 and self.first_head_uniform:\n",
    "                self.rel_embs[1] = 1\n",
    "            if self.mode == 'rel-norm':\n",
    "                self.rel_embs.fill_(0).add_(torch.randn(self.n_rels, self.emb_dims) * 0.1)\n",
    "\n",
    "        self.rel_embs = torch.nn.Parameter(self.rel_embs)\n",
    "\n",
    "        self.ew_embs = torch.nn.Parameter(torch.randn(self.n_rels, self.ew_hid_dims) *\n",
    "                                          (0.01 if self.mode == 'ment-norm' else 0.1))\n",
    "\n",
    "        self._coh_ctx_vecs = None\n",
    "\n",
    "        self.score_combine = torch.nn.Sequential(\n",
    "                torch.nn.Linear(2, self.hid_dims),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(self.hid_dims, 1))\n",
    "\n",
    "        print('---------------- model config -----------------')\n",
    "        for k, v in self.__dict__.items():\n",
    "            if not hasattr(v, '__dict__'):\n",
    "                print(k, v)\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "    def print_weight_norm(self):\n",
    "        LocalCtxAttRanker.print_weight_norm(self)\n",
    "        print(self.ctx_layer[0].weight.data.norm(), self.ctx_layer[0].bias.data.norm())\n",
    "        print('relations', self.rel_embs.data.norm(p=2, dim=1))\n",
    "        X = F.normalize(self.rel_embs)\n",
    "        diff = (X.view(self.n_rels, 1, -1) - X.view(1, self.n_rels, -1)).pow(2).sum(dim=2).sqrt()\n",
    "        print(diff)\n",
    "\n",
    "        print('ew_embs', self.ew_embs.data.norm(p=2, dim=1))\n",
    "        X = F.normalize(self.ew_embs)\n",
    "        diff = (X.view(self.n_rels, 1, -1) - X.view(1, self.n_rels, -1)).pow(2).sum(dim=2).sqrt()\n",
    "        print(diff)\n",
    "\n",
    "    def forward(self, token_ids, tok_mask, entity_ids, entity_mask, p_e_m, gold=None):\n",
    "        n_ments, n_cands = entity_ids.size()\n",
    "        n_rels = self.n_rels\n",
    "\n",
    "        if self.mode == 'ment-norm' and self.first_head_uniform:\n",
    "            self.ew_embs.data[0] = 0\n",
    "\n",
    "        if not self.oracle:\n",
    "            gold = None\n",
    "\n",
    "        if self.use_local:\n",
    "            local_ent_scores = super(MulRelRanker, self).forward(token_ids, tok_mask,\n",
    "                                                                 entity_ids, entity_mask,\n",
    "                                                                 p_e_m=None)\n",
    "            ent_vecs = self._entity_vecs\n",
    "        else:\n",
    "            ent_vecs = self.entity_embeddings(entity_ids)\n",
    "            local_ent_scores = Variable(torch.zeros(n_ments, n_cands).cuda(), requires_grad=False)\n",
    "\n",
    "        # compute context vectors\n",
    "        ltok_vecs = self.snd_word_embeddings(self.s_ltoken_ids) * self.s_ltoken_mask.view(n_ments, -1, 1)\n",
    "        local_lctx_vecs = torch.sum(ltok_vecs, dim=1) / torch.sum(self.s_ltoken_mask, dim=1, keepdim=True).add_(1e-5)\n",
    "        rtok_vecs = self.snd_word_embeddings(self.s_rtoken_ids) * self.s_rtoken_mask.view(n_ments, -1, 1)\n",
    "        local_rctx_vecs = torch.sum(rtok_vecs, dim=1) / torch.sum(self.s_rtoken_mask, dim=1, keepdim=True).add_(1e-5)\n",
    "        mtok_vecs = self.snd_word_embeddings(self.s_mtoken_ids) * self.s_mtoken_mask.view(n_ments, -1, 1)\n",
    "        ment_vecs = torch.sum(mtok_vecs, dim=1) / torch.sum(self.s_mtoken_mask, dim=1, keepdim=True).add_(1e-5)\n",
    "        bow_ctx_vecs = torch.cat([local_lctx_vecs, ment_vecs, local_rctx_vecs], dim=1)\n",
    "\n",
    "        if self.use_pad_ent:\n",
    "            ent_vecs = torch.cat([ent_vecs, self.pad_ent_emb.view(1, 1, -1).repeat(1, n_cands, 1)], dim=0)\n",
    "            tmp = torch.zeros(1, n_cands)\n",
    "            tmp[0, 0] = 1\n",
    "            tmp = Variable(tmp.cuda())\n",
    "            entity_mask = torch.cat([entity_mask, tmp], dim=0)\n",
    "            p_e_m = torch.cat([p_e_m, tmp], dim=0)\n",
    "            local_ent_scores = torch.cat([local_ent_scores,\n",
    "                                          Variable(torch.zeros(1, n_cands).cuda(), requires_grad=False)],\n",
    "                                         dim=0)\n",
    "            n_ments += 1\n",
    "\n",
    "            if self.oracle:\n",
    "                tmp = Variable(torch.zeros(1, 1).cuda().long())\n",
    "                gold = torch.cat([gold, tmp], dim=0)\n",
    "\n",
    "        if self.use_local_only:\n",
    "            inputs = torch.cat([Variable(torch.zeros(n_ments * n_cands, 1).cuda()),\n",
    "                                local_ent_scores.view(n_ments * n_cands, -1),\n",
    "                                torch.log(p_e_m + 1e-20).view(n_ments * n_cands, -1)], dim=1)\n",
    "            scores = self.score_combine(inputs).view(n_ments, n_cands)\n",
    "            return scores\n",
    "\n",
    "        if n_ments == 1:\n",
    "            ent_scores = local_ent_scores\n",
    "\n",
    "        else:\n",
    "            # distance - to consider only neighbor mentions\n",
    "            ment_pos = torch.arange(0, n_ments).long().cuda()\n",
    "            dist = (ment_pos.view(n_ments, 1) - ment_pos.view(1, n_ments)).abs()\n",
    "            dist.masked_fill_(dist == 1, -1)\n",
    "            dist.masked_fill_((dist > 1) & (dist <= self.max_dist), -1)\n",
    "            dist.masked_fill_(dist > self.max_dist, 0)\n",
    "            dist.mul_(-1)\n",
    "\n",
    "            ctx_vecs = self.ctx_layer(bow_ctx_vecs)\n",
    "            if self.use_pad_ent:\n",
    "                ctx_vecs = torch.cat([ctx_vecs, self.pad_ctx_vec], dim=0)\n",
    "\n",
    "            m1_ctx_vecs, m2_ctx_vecs = ctx_vecs, ctx_vecs\n",
    "            rel_ctx_vecs = m1_ctx_vecs.view(1, n_ments, -1) * self.ew_embs.view(n_rels, 1, -1)\n",
    "            rel_ctx_ctx_scores = torch.matmul(rel_ctx_vecs, m2_ctx_vecs.view(1, n_ments, -1).permute(0, 2, 1))  # n_rels x n_ments x n_ments\n",
    "\n",
    "            rel_ctx_ctx_scores = rel_ctx_ctx_scores.add_((1 - Variable(dist.float().cuda())).mul_(-1e10))\n",
    "            eye = Variable(torch.eye(n_ments).cuda()).view(1, n_ments, n_ments)\n",
    "            rel_ctx_ctx_scores.add_(eye.mul_(-1e10))\n",
    "            rel_ctx_ctx_scores.mul_(1 / np.sqrt(self.ew_hid_dims))  # scaling proposed by \"attention is all you need\"\n",
    "\n",
    "            # get top_n neighbour\n",
    "            if self.ent_top_n < n_ments:\n",
    "                topk_values, _ = torch.topk(rel_ctx_ctx_scores, k=min(self.ent_top_n, n_ments), dim=2)\n",
    "                threshold = topk_values[:, :, -1:]\n",
    "                mask = 1 - (rel_ctx_ctx_scores >= threshold).float()\n",
    "                rel_ctx_ctx_scores.add_(mask.mul_(-1e10))\n",
    "\n",
    "            if self.mode == 'ment-norm':\n",
    "                rel_ctx_ctx_probs = F.softmax(rel_ctx_ctx_scores, dim=2)\n",
    "                rel_ctx_ctx_weights = rel_ctx_ctx_probs + rel_ctx_ctx_probs.permute(0, 2, 1)\n",
    "                self._rel_ctx_ctx_weights = rel_ctx_ctx_probs\n",
    "            elif self.mode == 'rel-norm':\n",
    "                ctx_ctx_rel_scores = rel_ctx_ctx_scores.permute(1, 2, 0).contiguous()\n",
    "                if not self.use_stargmax:\n",
    "                    ctx_ctx_rel_probs = F.softmax(ctx_ctx_rel_scores.view(n_ments * n_ments, n_rels))\\\n",
    "                                        .view(n_ments, n_ments, n_rels)\n",
    "                else:\n",
    "                    ctx_ctx_rel_probs = STArgmax.apply(ctx_ctx_rel_scores)\n",
    "                self._rel_ctx_ctx_weights = ctx_ctx_rel_probs.permute(2, 0, 1).contiguous()\n",
    "\n",
    "            # compute phi(ei, ej)\n",
    "            if self.mode == 'ment-norm':\n",
    "                if self.ent_ent_comp == 'bilinear':\n",
    "                    if self.ent_ent_comp == 'bilinear':\n",
    "                        rel_ent_vecs = ent_vecs.view(1, n_ments, n_cands, -1) * self.rel_embs.view(n_rels, 1, 1, -1)\n",
    "                    elif self.ent_ent_comp == 'trans_e':\n",
    "                        rel_ent_vecs = ent_vecs.view(1, n_ments, n_cands, -1) - self.rel_embs.view(n_rels, 1, 1, -1)\n",
    "                    else:\n",
    "                        raise Exception(\"unknown ent_ent_comp\")\n",
    "\n",
    "                    rel_ent_ent_scores = torch.matmul(rel_ent_vecs.view(n_rels, n_ments, 1, n_cands, -1),\n",
    "                                                      ent_vecs.view(1, 1, n_ments, n_cands, -1).permute(0, 1, 2, 4, 3))\n",
    "                    # n_rels x n_ments x n_ments x n_cands x n_cands\n",
    "\n",
    "                rel_ent_ent_scores = rel_ent_ent_scores.permute(0, 1, 3, 2, 4)  # n_rel x n_ments x n_cands x n_ments x n_cands\n",
    "                rel_ent_ent_scores = (rel_ent_ent_scores * entity_mask).add_((entity_mask - 1).mul_(1e10))\n",
    "                ent_ent_scores = torch.sum(rel_ent_ent_scores *\n",
    "                                           rel_ctx_ctx_weights.view(n_rels, n_ments, 1, n_ments, 1), dim=0)\\\n",
    "                                 .mul(1. / n_rels)  # n_ments x n_cands x n_ments x n_cands\n",
    "\n",
    "            elif self.mode == 'rel-norm':\n",
    "                rel_vecs = torch.matmul(ctx_ctx_rel_probs.view(n_ments, n_ments, 1, n_rels),\n",
    "                                        self.rel_embs.view(1, 1, n_rels, -1))\\\n",
    "                           .view(n_ments, n_ments, -1)\n",
    "                ent_rel_vecs = ent_vecs.view(n_ments, 1, n_cands, -1) * rel_vecs.view(n_ments, n_ments, 1, -1)  # n_ments x n_ments x n_cands x dims\n",
    "                ent_ent_scores = torch.matmul(ent_rel_vecs,\n",
    "                                              ent_vecs.view(1, n_ments, n_cands, -1).permute(0, 1, 3, 2))\\\n",
    "                                 .permute(0, 2, 1, 3)\n",
    "\n",
    "            if gold is None:\n",
    "                # LBP\n",
    "                prev_msgs = Variable(torch.zeros(n_ments, n_cands, n_ments).cuda())\n",
    "\n",
    "                for _ in range(self.n_loops):\n",
    "                    mask = 1 - Variable(torch.eye(n_ments).cuda())\n",
    "                    ent_ent_votes = ent_ent_scores + local_ent_scores * 1 + \\\n",
    "                                    torch.sum(prev_msgs.view(1, n_ments, n_cands, n_ments) *\n",
    "                                              mask.view(n_ments, 1, 1, n_ments), dim=3)\\\n",
    "                                    .view(n_ments, 1, n_ments, n_cands)\n",
    "                    msgs, _ = torch.max(ent_ent_votes, dim=3)\n",
    "                    msgs = (F.softmax(msgs, dim=1).mul(self.dr) +\n",
    "                            prev_msgs.exp().mul(1 - self.dr)).log()\n",
    "                    prev_msgs = msgs\n",
    "\n",
    "                # compute marginal belief\n",
    "                mask = 1 - Variable(torch.eye(n_ments).cuda())\n",
    "                ent_scores = local_ent_scores * 1 + torch.sum(msgs * mask.view(n_ments, 1, n_ments), dim=2)\n",
    "                ent_scores = F.softmax(ent_scores, dim=1)\n",
    "            else:\n",
    "                onehot_gold = Variable(torch.zeros(n_ments, n_cands).cuda()).scatter_(1, gold, 1)\n",
    "                ent_scores = torch.sum(torch.sum(ent_ent_scores * onehot_gold, dim=3), dim=2)\n",
    "\n",
    "        # combine with p_e_m\n",
    "        inputs = torch.cat([ent_scores.view(n_ments * n_cands, -1),\n",
    "                            torch.log(p_e_m + 1e-20).view(n_ments * n_cands, -1)], dim=1)\n",
    "        scores = self.score_combine(inputs).view(n_ments, n_cands)\n",
    "\n",
    "        if self.use_pad_ent:\n",
    "            scores = scores[:-1]\n",
    "        return scores\n",
    "\n",
    "    def regularize(self, max_norm=1):\n",
    "        super(MulRelRanker, self).regularize(max_norm)\n",
    "\n",
    "    def loss(self, scores, true_pos, lamb=1e-7):\n",
    "        loss = F.multi_margin_loss(scores, true_pos, margin=self.margin)\n",
    "        if self.use_local_only:\n",
    "            return loss\n",
    "\n",
    "        # regularization\n",
    "        X = F.normalize(self.rel_embs)\n",
    "        diff = (X.view(self.n_rels, 1, -1) - X.view(1, self.n_rels, -1)).pow(2).sum(dim=2).add_(1e-5).sqrt()\n",
    "        diff = diff * (diff < 1).float()\n",
    "        loss -= torch.sum(diff).mul(lamb)\n",
    "\n",
    "        X = F.normalize(self.ew_embs)\n",
    "        diff = (X.view(self.n_rels, 1, -1) - X.view(1, self.n_rels, -1)).pow(2).sum(dim=2).add_(1e-5).sqrt()\n",
    "        diff = diff * (diff < 1).float()\n",
    "        loss -= torch.sum(diff).mul(lamb)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b81c6596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "wiki_link_prefix = 'http://en.wikipedia.org/wiki/'\n",
    "\n",
    "\n",
    "def read_csv_file(path):\n",
    "    data = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            comps = line.strip().split('\\t')\n",
    "            doc_name = comps[0] + ' ' + comps[1]\n",
    "            mention = comps[2]\n",
    "            lctx = comps[3]\n",
    "            rctx = comps[4]\n",
    "\n",
    "            if comps[6] != 'EMPTYCAND':\n",
    "                cands = [c.split(',') for c in comps[6:-2]]\n",
    "                cands = [(','.join(c[2:]).replace('\"', '%22').replace(' ', '_'), float(c[1])) for c in cands]\n",
    "            else:\n",
    "                cands = []\n",
    "\n",
    "            gold = comps[-1].split(',')\n",
    "            if gold[0] == '-1':\n",
    "                gold = (','.join(gold[2:]).replace('\"', '%22').replace(' ', '_'), 1e-5, -1)\n",
    "            else:\n",
    "                gold = (','.join(gold[3:]).replace('\"', '%22').replace(' ', '_'), 1e-5, -1)\n",
    "\n",
    "            if doc_name not in data:\n",
    "                data[doc_name] = []\n",
    "            data[doc_name].append(\n",
    "                {\n",
    "                    'mention': mention,\n",
    "                    'context': (lctx, rctx),\n",
    "                    'candidates': cands,\n",
    "                    'gold': gold\n",
    "                })\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_conll_file(data, path):\n",
    "    conll = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        cur_sent = None\n",
    "        cur_doc = None\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('-DOCSTART-'):\n",
    "                docname = line.split()[1][1:]\n",
    "                conll[docname] = {'sentences': [], 'mentions': []}\n",
    "                cur_doc = conll[docname]\n",
    "                cur_sent = []\n",
    "\n",
    "            else:\n",
    "                if line == '':\n",
    "                    cur_doc['sentences'].append(cur_sent)\n",
    "                    cur_sent = []\n",
    "\n",
    "                else:\n",
    "                    comps = line.split('\\t')\n",
    "                    tok = comps[0]\n",
    "                    cur_sent.append(tok)\n",
    "\n",
    "                    if len(comps) >=6 :\n",
    "                        bi = comps[1]\n",
    "                        wikilink = comps[4]\n",
    "                        if bi == 'I':\n",
    "                            cur_doc['mentions'][-1]['end'] += 1\n",
    "                        else:\n",
    "                            new_ment = {'sent_id': len(cur_doc['sentences']),\n",
    "                                        'start': len(cur_sent) - 1,\n",
    "                                        'end': len(cur_sent),\n",
    "                                        'wikilink': wikilink}\n",
    "                            cur_doc['mentions'].append(new_ment)\n",
    "\n",
    "    # merge with data\n",
    "    rmpunc = re.compile('[\\W]+')\n",
    "    for doc_name, content in data.items():\n",
    "        conll_doc = conll[doc_name.split()[0]]\n",
    "        content[0]['conll_doc'] = conll_doc\n",
    "\n",
    "        cur_conll_m_id = 0\n",
    "        for m in content:\n",
    "            mention = m['mention']\n",
    "            gold = m['gold']\n",
    "\n",
    "            while True:\n",
    "                cur_conll_m = conll_doc['mentions'][cur_conll_m_id]\n",
    "                cur_conll_mention = ' '.join(conll_doc['sentences'][cur_conll_m['sent_id']][cur_conll_m['start']:cur_conll_m['end']])\n",
    "                if rmpunc.sub('', cur_conll_mention.lower()) == rmpunc.sub('', mention.lower()):\n",
    "                    m['conll_m'] = cur_conll_m\n",
    "                    cur_conll_m_id += 1\n",
    "                    break\n",
    "                else:\n",
    "                    cur_conll_m_id += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_person_names(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            data.append(line.strip().replace(' ', '_'))\n",
    "    return set(data)\n",
    "\n",
    "\n",
    "def find_coref(ment, mentlist, person_names):\n",
    "    cur_m = ment['mention'].lower()\n",
    "    coref = []\n",
    "    for m in mentlist:\n",
    "        if len(m['candidates']) == 0 or m['candidates'][0][0] not in person_names:\n",
    "            continue\n",
    "\n",
    "        mention = m['mention'].lower()\n",
    "        start_pos = mention.find(cur_m)\n",
    "        if start_pos == -1 or mention == cur_m:\n",
    "            continue\n",
    "\n",
    "        end_pos = start_pos + len(cur_m) - 1\n",
    "        if (start_pos == 0 or mention[start_pos-1] == ' ') and \\\n",
    "                (end_pos == len(mention) - 1 or mention[end_pos + 1] == ' '):\n",
    "            coref.append(m)\n",
    "\n",
    "    return coref\n",
    "\n",
    "\n",
    "def with_coref(dataset, person_names):\n",
    "    for data_name, content in dataset.items():\n",
    "        for cur_m in content:\n",
    "            coref = find_coref(cur_m, content, person_names)\n",
    "            if coref is not None and len(coref) > 0:\n",
    "                cur_cands = {}\n",
    "                for m in coref:\n",
    "                    for c, p in m['candidates']:\n",
    "                        cur_cands[c] = cur_cands.get(c, 0) + p\n",
    "                for c in cur_cands.keys():\n",
    "                    cur_cands[c] /= len(coref)\n",
    "                cur_m['candidates'] = sorted(list(cur_cands.items()), key=lambda x: x[1])[::-1]\n",
    "\n",
    "\n",
    "def dataset_eval(testset, system_pred):\n",
    "    gold = []\n",
    "    pred = []\n",
    "\n",
    "    for doc_name, content in testset.items():\n",
    "        gold += [c['gold'][0] for c in content]\n",
    "        pred += [c['pred'][0] for c in system_pred[doc_name]]\n",
    "\n",
    "    true_pos = 0\n",
    "    for g, p in zip(gold, pred):\n",
    "        if g == p and p != 'NIL':\n",
    "            true_pos += 1\n",
    "\n",
    "    precision = true_pos / len([p for p in pred if p != 'NIL'])\n",
    "    recall = true_pos / len(gold)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "class CoNLLDataset:\n",
    "    \"\"\"\n",
    "    reading dataset from CoNLL dataset, extracted by https://github.com/dalab/deep-ed/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, person_path, conll_path):\n",
    "        print('load csv')\n",
    "        self.train = read_csv_file(path + '/aida_train.csv')\n",
    "        self.testA = read_csv_file(path + '/aida_testA.csv')\n",
    "        self.testB = read_csv_file(path + '/aida_testB.csv')\n",
    "        self.ace2004 = read_csv_file(path + '/wned-ace2004.csv')\n",
    "        self.aquaint = read_csv_file(path + '/wned-aquaint.csv')\n",
    "        self.clueweb = read_csv_file(path + '/wned-clueweb.csv')\n",
    "        self.msnbc = read_csv_file(path + '/wned-msnbc.csv')\n",
    "        self.wikipedia = read_csv_file(path + '/wned-wikipedia.csv')\n",
    "        self.wikipedia.pop('Jiří_Třanovský Jiří_Třanovský', None)\n",
    "\n",
    "        print('process coref')\n",
    "        person_names = load_person_names(person_path)\n",
    "        with_coref(self.train, person_names)\n",
    "        with_coref(self.testA, person_names)\n",
    "        with_coref(self.testB, person_names)\n",
    "        with_coref(self.ace2004, person_names)\n",
    "        with_coref(self.aquaint, person_names)\n",
    "        with_coref(self.clueweb, person_names)\n",
    "        with_coref(self.msnbc, person_names)\n",
    "        with_coref(self.wikipedia, person_names)\n",
    "\n",
    "        print('load conll')\n",
    "        read_conll_file(self.train, conll_path + '/AIDA/aida_train.txt')\n",
    "        read_conll_file(self.testA, conll_path + '/AIDA/testa_testb_aggregate_original')\n",
    "        read_conll_file(self.testB, conll_path + '/AIDA/testa_testb_aggregate_original')\n",
    "        read_conll_file(self.ace2004, conll_path + '/wned-datasets/ace2004/ace2004.conll')\n",
    "        read_conll_file(self.aquaint, conll_path + '/wned-datasets/aquaint/aquaint.conll')\n",
    "        read_conll_file(self.msnbc, conll_path + '/wned-datasets/msnbc/msnbc.conll')\n",
    "        read_conll_file(self.clueweb, conll_path + '/wned-datasets/clueweb/clueweb.conll')\n",
    "        read_conll_file(self.wikipedia, conll_path + '/wned-datasets/wikipedia/wikipedia.conll')\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    path = '/datastore/ple/workspace/nel/preprocess_data/data/generated/test_train_data/'\n",
    "#    conll_path = '/datastore/ple/workspace/nel/preprocess_data/data/basic_data/test_datasets/'\n",
    "#    person_path = '/datastore/ple/workspace/nel/preprocess_data/data/basic_data/p_e_m_data/persons.txt'\n",
    "\n",
    "#    dataset = CoNLLDataset(path, person_path, conll_path)\n",
    "    # from pprint import pprint\n",
    "    # pprint(dataset.ace2004, width=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0992d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NTEE(AbstractWordEntity):\n",
    "    \"\"\"\n",
    "    NTEE model, proposed in Yamada et al. \"Learning Distributed Representations of Texts and Entities from Knowledge Base\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        config['word_embeddings_class'] = nn.EmbeddingBag\n",
    "        config['entity_embeddings_class'] = nn.Embedding\n",
    "        super(NTEE, self).__init__(config)\n",
    "        self.linear = nn.Linear(self.emb_dims, self.emb_dims)\n",
    "\n",
    "    def compute_sent_vecs(self, token_ids, token_offsets, use_sum=False):\n",
    "        sum_vecs = self.word_embeddings(token_ids, token_offsets)\n",
    "        if use_sum:\n",
    "            return sum_vecs\n",
    "\n",
    "        sum_vecs = F.normalize(sum_vecs)\n",
    "        sent_vecs = self.linear(sum_vecs)\n",
    "        return sent_vecs\n",
    "\n",
    "    def forward(self, token_ids, token_offsets, entity_ids, use_sum=False):\n",
    "        sent_vecs = self.compute_sent_vecs(token_ids, token_offsets, use_sum)\n",
    "        entity_vecs = self.entity_embeddings(entity_ids)\n",
    "\n",
    "        # compute scores\n",
    "        batchsize, dims = sent_vecs.size()\n",
    "        n_entities = entity_vecs.size(1)\n",
    "        scores = torch.bmm(entity_vecs, sent_vecs.view(batchsize, dims, 1)).view(batchsize, n_entities)\n",
    "\n",
    "        log_probs = F.log_softmax(scores, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "    def predict(self, token_ids, token_offsets, entity_ids, gold_entity_ids=None):\n",
    "        log_probs = self.forward(token_ids, token_offsets, entity_ids)\n",
    "        _, pred_entity_ids = torch.max(log_probs, dim=1)\n",
    "\n",
    "        acc = None\n",
    "        if gold_entity_ids is not None:\n",
    "            acc = torch.eq(gold_entity_ids, pred_entity_ids).sum()\n",
    "        return pred_entity_ids, acc\n",
    "\n",
    "    def loss(self, log_probs, true_pos):\n",
    "        return F.nll_loss(log_probs, true_pos)\n",
    "\n",
    "\n",
    "def create_ntee_from_components(dir_path):\n",
    "    word_dict_path = dir_path + '/dict.word'\n",
    "    word_embs_path = dir_path + '/word_embeddings.npy'\n",
    "    entity_dict_path = dir_path + '/dict.entity'\n",
    "    entity_embs_path = dir_path + '/entity_embeddings.npy'\n",
    "    W_path = dir_path + '/W.npy'\n",
    "    b_path = dir_path + '/b.npy'\n",
    "\n",
    "    print('load voca and embeddings')\n",
    "    word_voca, word_embs = load_voca_embs(word_dict_path, word_embs_path)\n",
    "    entity_voca, entity_embs = load_voca_embs(entity_dict_path, entity_embs_path)\n",
    "    config = {'word_embeddings': word_embs,\n",
    "              'entity_embeddings': entity_embs,\n",
    "              'word_voca':  word_voca,\n",
    "              'entity_voca': entity_voca,\n",
    "              'emb_dims': word_embs.shape[1]}\n",
    "    print(word_embs.shape, entity_embs.shape)\n",
    "\n",
    "    # create model\n",
    "    print('create model')\n",
    "    model = NTEE(config)\n",
    "\n",
    "    W = np.load(W_path)\n",
    "    b = np.load(b_path)\n",
    "    model.linear.weight = nn.Parameter(torch.FloatTensor(W).t())\n",
    "    model.linear.bias = nn.Parameter(torch.FloatTensor(b))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b10c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ed ranker\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "from random import shuffle\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "ModelClass = MulRelRanker\n",
    "wiki_prefix = 'en.wikipedia.org/wiki/'\n",
    "\n",
    "\n",
    "class EDRanker:\n",
    "    \"\"\"\n",
    "    ranking candidates\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        print('--- create model ---')\n",
    "\n",
    "        config['entity_embeddings'] = config['entity_embeddings'] / \\\n",
    "                                      np.maximum(np.linalg.norm(config['entity_embeddings'],\n",
    "                                                                axis=1, keepdims=True), 1e-12)\n",
    "        config['entity_embeddings'][config['entity_voca'].unk_id] = 1e-10\n",
    "        config['word_embeddings'] = config['word_embeddings'] / \\\n",
    "                                    np.maximum(np.linalg.norm(config['word_embeddings'],\n",
    "                                                              axis=1, keepdims=True), 1e-12)\n",
    "        config['word_embeddings'][config['word_voca'].unk_id] = 1e-10\n",
    "\n",
    "        print('prerank model')\n",
    "        self.prerank_model = NTEE(config)\n",
    "        self.args = config['args']\n",
    "\n",
    "        print('main model')\n",
    "        if self.args.mode == 'eval':\n",
    "            print('try loading model from', self.args.model_path)\n",
    "            self.model = load_model(self.args.model_path, ModelClass)\n",
    "        else:\n",
    "            print('create new model')\n",
    "            if config['mulrel_type'] == 'rel-norm':\n",
    "                config['use_stargmax'] = False\n",
    "            if config['mulrel_type'] == 'ment-norm':\n",
    "                config['first_head_uniform'] = False\n",
    "                config['use_pad_ent'] = True\n",
    "\n",
    "            config['use_local'] = True\n",
    "            config['use_local_only'] = False\n",
    "            config['oracle'] = False\n",
    "            self.model = ModelClass(config)\n",
    "\n",
    "        self.prerank_model.cuda()\n",
    "        self.model.cuda()\n",
    "\n",
    "    def prerank(self, dataset, predict=False):\n",
    "        new_dataset = []\n",
    "        has_gold = 0\n",
    "        total = 0\n",
    "\n",
    "        for content in dataset:\n",
    "            items = []\n",
    "\n",
    "            if self.args.keep_ctx_ent > 0:\n",
    "                # rank the candidates by ntee scores\n",
    "                lctx_ids = [m['context'][0][max(len(m['context'][0]) - self.args.prerank_ctx_window // 2, 0):]\n",
    "                            for m in content]\n",
    "                rctx_ids = [m['context'][1][:min(len(m['context'][1]), self.args.prerank_ctx_window // 2)]\n",
    "                            for m in content]\n",
    "                ment_ids = [[] for m in content]\n",
    "                token_ids = [l + m + r if len(l) + len(r) > 0 else [self.prerank_model.word_voca.unk_id]\n",
    "                             for l, m, r in zip(lctx_ids, ment_ids, rctx_ids)]\n",
    "\n",
    "                entity_ids = [m['cands'] for m in content]\n",
    "                entity_ids = Variable(torch.LongTensor(entity_ids).cuda())\n",
    "\n",
    "                entity_mask = [m['mask'] for m in content]\n",
    "                entity_mask = Variable(torch.FloatTensor(entity_mask).cuda())\n",
    "\n",
    "                token_ids, token_offsets = flatten_list_of_lists(token_ids)\n",
    "                token_offsets = Variable(torch.LongTensor(token_offsets).cuda())\n",
    "                token_ids = Variable(torch.LongTensor(token_ids).cuda())\n",
    "\n",
    "                log_probs = self.prerank_model.forward(token_ids, token_offsets, entity_ids, use_sum=True)\n",
    "                log_probs = (log_probs * entity_mask).add_((entity_mask - 1).mul_(1e10))\n",
    "                _, top_pos = torch.topk(log_probs, dim=1, k=self.args.keep_ctx_ent)\n",
    "                top_pos = top_pos.data.cpu().numpy()\n",
    "\n",
    "            else:\n",
    "                top_pos = [[]] * len(content)\n",
    "\n",
    "            # select candidats: mix between keep_ctx_ent best candidates (ntee scores) with\n",
    "            # keep_p_e_m best candidates (p_e_m scores)\n",
    "            for i, m in enumerate(content):\n",
    "                sm = {'cands': [],\n",
    "                      'named_cands': [],\n",
    "                      'p_e_m': [],\n",
    "                      'mask': [],\n",
    "                      'true_pos': -1}\n",
    "                m['selected_cands'] = sm\n",
    "\n",
    "                selected = set(top_pos[i])\n",
    "                idx = 0\n",
    "                while len(selected) < self.args.keep_ctx_ent + self.args.keep_p_e_m:\n",
    "                    if idx not in selected:\n",
    "                        selected.add(idx)\n",
    "                    idx += 1\n",
    "\n",
    "                selected = sorted(list(selected))\n",
    "                for idx in selected:\n",
    "                    sm['cands'].append(m['cands'][idx])\n",
    "                    sm['named_cands'].append(m['named_cands'][idx])\n",
    "                    sm['p_e_m'].append(m['p_e_m'][idx])\n",
    "                    sm['mask'].append(m['mask'][idx])\n",
    "                    if idx == m['true_pos']:\n",
    "                        sm['true_pos'] = len(sm['cands']) - 1\n",
    "\n",
    "                if not predict:\n",
    "                    if sm['true_pos'] == -1:\n",
    "                        continue\n",
    "\n",
    "                items.append(m)\n",
    "                if sm['true_pos'] >= 0:\n",
    "                    has_gold += 1\n",
    "                total += 1\n",
    "\n",
    "                if predict:\n",
    "                    # only for oracle model, not used for eval\n",
    "                    if sm['true_pos'] == -1:\n",
    "                        sm['true_pos'] = 0  # a fake gold, happens only 2%, but avoid the non-gold\n",
    "\n",
    "            if len(items) > 0:\n",
    "                new_dataset.append(items)\n",
    "\n",
    "        print('recall', has_gold / total)\n",
    "        return new_dataset\n",
    "\n",
    "    def get_data_items(self, dataset, predict=False):\n",
    "        data = []\n",
    "        cand_source = 'candidates'\n",
    "\n",
    "        for doc_name, content in dataset.items():\n",
    "            items = []\n",
    "            conll_doc = content[0].get('conll_doc', None)\n",
    "\n",
    "            for m in content:\n",
    "                try:\n",
    "                    named_cands = [c[0] for c in m[cand_source]]\n",
    "                    p_e_m = [min(1., max(1e-3, c[1])) for c in m[cand_source]]\n",
    "                except:\n",
    "                    named_cands = [c[0] for c in m['candidates']]\n",
    "                    p_e_m = [min(1., max(1e-3, c[1])) for c in m['candidates']]\n",
    "\n",
    "                try:\n",
    "                    true_pos = named_cands.index(m['gold'][0])\n",
    "                    p = p_e_m[true_pos]\n",
    "                except:\n",
    "                    true_pos = -1\n",
    "\n",
    "                named_cands = named_cands[:min(self.args.n_cands_before_rank, len(named_cands))]\n",
    "                p_e_m = p_e_m[:min(self.args.n_cands_before_rank, len(p_e_m))]\n",
    "\n",
    "                if true_pos >= len(named_cands):\n",
    "                    if not predict:\n",
    "                        true_pos = len(named_cands) - 1\n",
    "                        p_e_m[-1] = p\n",
    "                        named_cands[-1] = m['gold'][0]\n",
    "                    else:\n",
    "                        true_pos = -1\n",
    "\n",
    "                cands = [self.model.entity_voca.get_id(wiki_prefix + c) for c in named_cands]\n",
    "                mask = [1.] * len(cands)\n",
    "                if len(cands) == 0 and not predict:\n",
    "                    continue\n",
    "                elif len(cands) < self.args.n_cands_before_rank:\n",
    "                    cands += [self.model.entity_voca.unk_id] * (self.args.n_cands_before_rank - len(cands))\n",
    "                    named_cands += [Vocabulary.unk_token] * (self.args.n_cands_before_rank - len(named_cands))\n",
    "                    p_e_m += [1e-8] * (self.args.n_cands_before_rank - len(p_e_m))\n",
    "                    mask += [0.] * (self.args.n_cands_before_rank - len(mask))\n",
    "\n",
    "                lctx = m['context'][0].strip().split()\n",
    "                lctx_ids = [self.prerank_model.word_voca.get_id(t) for t in lctx if is_important_word(t)]\n",
    "                lctx_ids = [tid for tid in lctx_ids if tid != self.prerank_model.word_voca.unk_id]\n",
    "                lctx_ids = lctx_ids[max(0, len(lctx_ids) - self.args.ctx_window//2):]\n",
    "\n",
    "                rctx = m['context'][1].strip().split()\n",
    "                rctx_ids = [self.prerank_model.word_voca.get_id(t) for t in rctx if is_important_word(t)]\n",
    "                rctx_ids = [tid for tid in rctx_ids if tid != self.prerank_model.word_voca.unk_id]\n",
    "                rctx_ids = rctx_ids[:min(len(rctx_ids), self.args.ctx_window//2)]\n",
    "\n",
    "                ment = m['mention'].strip().split()\n",
    "                ment_ids = [self.prerank_model.word_voca.get_id(t) for t in ment if is_important_word(t)]\n",
    "                ment_ids = [tid for tid in ment_ids if tid != self.prerank_model.word_voca.unk_id]\n",
    "\n",
    "                m['sent'] = ' '.join(lctx + rctx)\n",
    "\n",
    "                # secondary local context (for computing relation scores)\n",
    "                if conll_doc is not None:\n",
    "                    conll_m = m['conll_m']\n",
    "                    sent = conll_doc['sentences'][conll_m['sent_id']]\n",
    "                    start = conll_m['start']\n",
    "                    end = conll_m['end']\n",
    "\n",
    "                    snd_lctx = [self.model.snd_word_voca.get_id(t)\n",
    "                                for t in sent[max(0, start - self.args.snd_local_ctx_window//2):start]]\n",
    "                    snd_rctx = [self.model.snd_word_voca.get_id(t)\n",
    "                                for t in sent[end:min(len(sent), end + self.args.snd_local_ctx_window//2)]]\n",
    "                    snd_ment = [self.model.snd_word_voca.get_id(t)\n",
    "                                for t in sent[start:end]]\n",
    "\n",
    "                    if len(snd_lctx) == 0:\n",
    "                        snd_lctx = [self.model.snd_word_voca.unk_id]\n",
    "                    if len(snd_rctx) == 0:\n",
    "                        snd_rctx = [self.model.snd_word_voca.unk_id]\n",
    "                    if len(snd_ment) == 0:\n",
    "                        snd_ment = [self.model.snd_word_voca.unk_id]\n",
    "                else:\n",
    "                    snd_lctx = [self.model.snd_word_voca.unk_id]\n",
    "                    snd_rctx = [self.model.snd_word_voca.unk_id]\n",
    "                    snd_ment = [self.model.snd_word_voca.unk_id]\n",
    "\n",
    "                items.append({'context': (lctx_ids, rctx_ids),\n",
    "                              'snd_ctx': (snd_lctx, snd_rctx),\n",
    "                              'ment_ids': ment_ids,\n",
    "                              'snd_ment': snd_ment,\n",
    "                              'cands': cands,\n",
    "                              'named_cands': named_cands,\n",
    "                              'p_e_m': p_e_m,\n",
    "                              'mask': mask,\n",
    "                              'true_pos': true_pos,\n",
    "                              'doc_name': doc_name,\n",
    "                              'raw': m\n",
    "                              })\n",
    "\n",
    "            if len(items) > 0:\n",
    "                # note: this shouldn't affect the order of prediction because we use doc_name to add predicted entities,\n",
    "                # and we don't shuffle the data for prediction\n",
    "                if len(items) > 100:\n",
    "                    print(len(items))\n",
    "                    for k in range(0, len(items), 100):\n",
    "                        data.append(items[k:min(len(items), k + 100)])\n",
    "                else:\n",
    "                    data.append(items)\n",
    "\n",
    "        return self.prerank(data, predict)\n",
    "\n",
    "    def train(self, org_train_dataset, org_dev_datasets, config):\n",
    "        print('extracting training data')\n",
    "        train_dataset = self.get_data_items(org_train_dataset, predict=False)\n",
    "        print('#train docs', len(train_dataset))\n",
    "\n",
    "        dev_datasets = []\n",
    "        for dname, data in org_dev_datasets:\n",
    "            dev_datasets.append((dname, self.get_data_items(data, predict=True)))\n",
    "            print(dname, '#dev docs', len(dev_datasets[-1][1]))\n",
    "\n",
    "        print('creating optimizer')\n",
    "        optimizer = optim.Adam([p for p in self.model.parameters() if p.requires_grad], lr=config['lr'])\n",
    "        best_f1 = -1\n",
    "        not_better_count = 0\n",
    "        is_counting = False\n",
    "        eval_after_n_epochs = self.args.eval_after_n_epochs\n",
    "\n",
    "        for e in range(config['n_epochs']):\n",
    "            shuffle(train_dataset)\n",
    "\n",
    "            total_loss = 0\n",
    "            for dc, batch in enumerate(train_dataset):  # each document is a minibatch\n",
    "                self.model.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # convert data items to pytorch inputs\n",
    "                token_ids = [m['context'][0] + m['context'][1]\n",
    "                             if len(m['context'][0]) + len(m['context'][1]) > 0\n",
    "                             else [self.model.word_voca.unk_id]\n",
    "                             for m in batch]\n",
    "                s_ltoken_ids = [m['snd_ctx'][0] for m in batch]\n",
    "                s_rtoken_ids = [m['snd_ctx'][1] for m in batch]\n",
    "                s_mtoken_ids = [m['snd_ment'] for m in batch]\n",
    "\n",
    "                entity_ids = Variable(torch.LongTensor([m['selected_cands']['cands'] for m in batch]).cuda())\n",
    "                true_pos = Variable(torch.LongTensor([m['selected_cands']['true_pos'] for m in batch]).cuda())\n",
    "                p_e_m = Variable(torch.FloatTensor([m['selected_cands']['p_e_m'] for m in batch]).cuda())\n",
    "                entity_mask = Variable(torch.FloatTensor([m['selected_cands']['mask'] for m in batch]).cuda())\n",
    "\n",
    "                token_ids, token_mask = make_equal_len(token_ids, self.model.word_voca.unk_id)\n",
    "                s_ltoken_ids, s_ltoken_mask = make_equal_len(s_ltoken_ids, self.model.snd_word_voca.unk_id,\n",
    "                                                                   to_right=False)\n",
    "                s_rtoken_ids, s_rtoken_mask = make_equal_len(s_rtoken_ids, self.model.snd_word_voca.unk_id)\n",
    "                s_rtoken_ids = [l[::-1] for l in s_rtoken_ids]\n",
    "                s_rtoken_mask = [l[::-1] for l in s_rtoken_mask]\n",
    "                s_mtoken_ids, s_mtoken_mask = make_equal_len(s_mtoken_ids, self.model.snd_word_voca.unk_id)\n",
    "\n",
    "                token_ids = Variable(torch.LongTensor(token_ids).cuda())\n",
    "                token_mask = Variable(torch.FloatTensor(token_mask).cuda())\n",
    "                # too ugly but too lazy to fix it\n",
    "                self.model.s_ltoken_ids = Variable(torch.LongTensor(s_ltoken_ids).cuda())\n",
    "                self.model.s_ltoken_mask = Variable(torch.FloatTensor(s_ltoken_mask).cuda())\n",
    "                self.model.s_rtoken_ids = Variable(torch.LongTensor(s_rtoken_ids).cuda())\n",
    "                self.model.s_rtoken_mask = Variable(torch.FloatTensor(s_rtoken_mask).cuda())\n",
    "                self.model.s_mtoken_ids = Variable(torch.LongTensor(s_mtoken_ids).cuda())\n",
    "                self.model.s_mtoken_mask = Variable(torch.FloatTensor(s_mtoken_mask).cuda())\n",
    "\n",
    "                scores = self.model.forward(token_ids, token_mask, entity_ids, entity_mask, p_e_m,\n",
    "                                            gold=true_pos.view(-1, 1))\n",
    "                loss = self.model.loss(scores, true_pos)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                self.model.regularize(max_norm=100)\n",
    "\n",
    "                loss = loss.cpu().data.numpy()\n",
    "                total_loss += loss\n",
    "                print('epoch', e, \"%0.2f%%\" % (dc/len(train_dataset) * 100), loss, end='\\r')\n",
    "\n",
    "            print('epoch', e, 'total loss', total_loss, total_loss / len(train_dataset))\n",
    "\n",
    "            if (e + 1) % eval_after_n_epochs == 0:\n",
    "                dev_f1 = 0\n",
    "                for di, (dname, data) in enumerate(dev_datasets):\n",
    "                    predictions = self.predict(data)\n",
    "                    f1 = dataset_eval(org_dev_datasets[di][1], predictions)\n",
    "                    print(dname, tokgreen('micro F1: ' + str(f1)))\n",
    "\n",
    "                    if dname == 'aida-A':\n",
    "                        dev_f1 = f1\n",
    "\n",
    "                if config['lr'] == 1e-4 and dev_f1 >= self.args.dev_f1_change_lr:\n",
    "                    eval_after_n_epochs = 2\n",
    "                    is_counting = True\n",
    "                    best_f1 = dev_f1\n",
    "                    not_better_count = 0\n",
    "\n",
    "                    config['lr'] = 1e-5\n",
    "                    print('change learning rate to', config['lr'])\n",
    "                    if self.args.mulrel_type == 'rel-norm':\n",
    "                        optimizer = optim.Adam([p for p in self.model.parameters() if p.requires_grad], lr=config['lr'])\n",
    "                    elif self.args.mulrel_type == 'ment-norm':\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = config['lr']\n",
    "\n",
    "                if is_counting:\n",
    "                    if dev_f1 < best_f1:\n",
    "                        not_better_count += 1\n",
    "                    else:\n",
    "                        not_better_count = 0\n",
    "                        best_f1 = dev_f1\n",
    "                        print('save model to', self.args.model_path)\n",
    "                        self.model.save(self.args.model_path)\n",
    "\n",
    "                if not_better_count == self.args.n_not_inc:\n",
    "                    break\n",
    "\n",
    "                self.model.print_weight_norm()\n",
    "\n",
    "    def predict(self, data):\n",
    "        predictions = {items[0]['doc_name']: [] for items in data}\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch in data:  # each document is a minibatch\n",
    "            token_ids = [m['context'][0] + m['context'][1]\n",
    "                         if len(m['context'][0]) + len(m['context'][1]) > 0\n",
    "                         else [self.model.word_voca.unk_id]\n",
    "                         for m in batch]\n",
    "            s_ltoken_ids = [m['snd_ctx'][0] for m in batch]\n",
    "            s_rtoken_ids = [m['snd_ctx'][1] for m in batch]\n",
    "            s_mtoken_ids = [m['snd_ment'] for m in batch]\n",
    "\n",
    "            lctx_ids = s_ltoken_ids\n",
    "            rctx_ids = s_rtoken_ids\n",
    "            m_ids = s_mtoken_ids\n",
    "\n",
    "            entity_ids = Variable(torch.LongTensor([m['selected_cands']['cands'] for m in batch]).cuda())\n",
    "            p_e_m = Variable(torch.FloatTensor([m['selected_cands']['p_e_m'] for m in batch]).cuda())\n",
    "            entity_mask = Variable(torch.FloatTensor([m['selected_cands']['mask'] for m in batch]).cuda())\n",
    "            true_pos = Variable(torch.LongTensor([m['selected_cands']['true_pos'] for m in batch]).cuda())\n",
    "\n",
    "            token_ids, token_mask = make_equal_len(token_ids, self.model.word_voca.unk_id)\n",
    "            s_ltoken_ids, s_ltoken_mask = make_equal_len(s_ltoken_ids, self.model.snd_word_voca.unk_id,\n",
    "                                                               to_right=False)\n",
    "            s_rtoken_ids, s_rtoken_mask = make_equal_len(s_rtoken_ids, self.model.snd_word_voca.unk_id)\n",
    "            s_rtoken_ids = [l[::-1] for l in s_rtoken_ids]\n",
    "            s_rtoken_mask = [l[::-1] for l in s_rtoken_mask]\n",
    "            s_mtoken_ids, s_mtoken_mask = make_equal_len(s_mtoken_ids, self.model.snd_word_voca.unk_id)\n",
    "\n",
    "            token_ids = Variable(torch.LongTensor(token_ids).cuda())\n",
    "            token_mask = Variable(torch.FloatTensor(token_mask).cuda())\n",
    "            # too ugly, but too lazy to fix it\n",
    "            self.model.s_ltoken_ids = Variable(torch.LongTensor(s_ltoken_ids).cuda())\n",
    "            self.model.s_ltoken_mask = Variable(torch.FloatTensor(s_ltoken_mask).cuda())\n",
    "            self.model.s_rtoken_ids = Variable(torch.LongTensor(s_rtoken_ids).cuda())\n",
    "            self.model.s_rtoken_mask = Variable(torch.FloatTensor(s_rtoken_mask).cuda())\n",
    "            self.model.s_mtoken_ids = Variable(torch.LongTensor(s_mtoken_ids).cuda())\n",
    "            self.model.s_mtoken_mask = Variable(torch.FloatTensor(s_mtoken_mask).cuda())\n",
    "\n",
    "            scores = self.model.forward(token_ids, token_mask, entity_ids, entity_mask, p_e_m,\n",
    "                                        gold=true_pos.view(-1, 1))\n",
    "            scores = scores.cpu().data.numpy()\n",
    "\n",
    "            # print out relation weights\n",
    "            if self.args.mode == 'eval' and self.args.print_rel:\n",
    "                print('================================')\n",
    "                weights = self.model._rel_ctx_ctx_weights.cpu().data.numpy()\n",
    "                voca = self.model.snd_word_voca\n",
    "                for i in range(len(batch)):\n",
    "                    print(' '.join([voca.id2word[id] for id in lctx_ids[i]]),\n",
    "                          tokgreen(' '.join([voca.id2word[id] for id in m_ids[i]])),\n",
    "                          ' '.join([voca.id2word[id] for id in rctx_ids[i]]))\n",
    "                    for j in range(len(batch)):\n",
    "                        if i == j:\n",
    "                            continue\n",
    "                        np.set_printoptions(precision=2)\n",
    "                        print('\\t', weights[:, i, j], '\\t',\n",
    "                              ' '.join([voca.id2word[id] for id in lctx_ids[j]]),\n",
    "                              tokgreen(' '.join([voca.id2word[id] for id in m_ids[j]])),\n",
    "                              ' '.join([voca.id2word[id] for id in rctx_ids[j]]))\n",
    "\n",
    "            pred_ids = np.argmax(scores, axis=1)\n",
    "            pred_entities = [m['selected_cands']['named_cands'][i] if m['selected_cands']['mask'][i] == 1\n",
    "                             else (m['selected_cands']['named_cands'][0] if m['selected_cands']['mask'][0] == 1 else 'NIL')\n",
    "                             for (i, m) in zip(pred_ids, batch)]\n",
    "            doc_names = [m['doc_name'] for m in batch]\n",
    "\n",
    "            if self.args.mode == 'eval' and self.args.print_incorrect:\n",
    "                gold = [item['selected_cands']['named_cands'][item['selected_cands']['true_pos']]\n",
    "                        if item['selected_cands']['true_pos'] >= 0 else 'UNKNOWN' for item in batch]\n",
    "                pred = pred_entities\n",
    "                for i in range(len(gold)):\n",
    "                    if gold[i] != pred[i]:\n",
    "                        print('--------------------------------------------')\n",
    "                        pprint(batch[i]['raw'])\n",
    "                        print(gold[i], pred[i])\n",
    "\n",
    "            for dname, entity in zip(doc_names, pred_entities):\n",
    "                predictions[dname].append({'pred': (entity, 0.)})\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35e5915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load core voca from -f\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m word_embs_dir \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload core voca from\u001b[39m\u001b[38;5;124m'\u001b[39m, core_voca_path)\n\u001b[1;32m---> 12\u001b[0m core_voca \u001b[38;5;241m=\u001b[39m \u001b[43mVocabulary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcore_voca_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload full voca and embs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m full_voca, full_embs \u001b[38;5;241m=\u001b[39m load_voca_embs(\n\u001b[0;32m     16\u001b[0m     word_embs_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/all_dict.word\u001b[39m\u001b[38;5;124m'\u001b[39m, word_embs_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/all_word_embeddings.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 38\u001b[0m, in \u001b[0;36mVocabulary.load\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(path):\n\u001b[0;32m     37\u001b[0m     voca \u001b[38;5;241m=\u001b[39m Vocabulary()\n\u001b[1;32m---> 38\u001b[0m     \u001b[43mvoca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m voca\n",
      "Cell \u001b[1;32mIn[2], line 46\u001b[0m, in \u001b[0;36mVocabulary.load_from_file\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2word \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 46\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m     48\u001b[0m     line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "##filter word2vec\n",
    "\n",
    "import sys\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "\n",
    "core_voca_path = sys.argv[1]\n",
    "word_embs_dir = sys.argv[2]\n",
    "\n",
    "print('load core voca from', core_voca_path)\n",
    "core_voca = Vocabulary.load(core_voca_path)\n",
    "\n",
    "print('load full voca and embs')\n",
    "full_voca, full_embs = load_voca_embs(\n",
    "    word_embs_dir + '/all_dict.word', word_embs_dir + '/all_word_embeddings.npy')\n",
    "\n",
    "print('select word ids')\n",
    "selected = []\n",
    "for word in core_voca.id2word: \n",
    "    word_id = full_voca.word2id.get(word, -1)\n",
    "    if word_id >= 0: \n",
    "        selected.append(word_id)\n",
    "\n",
    "print('save...')\n",
    "selected_embs = full_embs[selected, :]\n",
    "np.save(word_embs_dir + '/word_embeddings', selected_embs)\n",
    "\n",
    "with open(word_embs_dir + '/dict.word', 'w', encoding='utf8') as f:\n",
    "    for i in selected: \n",
    "        f.write(full_voca.id2word[i] + '\\t1000\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67dbb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LocalCtxAttRanker(AbstractWordEntity):\n",
    "    \"\"\"\n",
    "    local model with context token attention (from G&H's EMNLP paper)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        config['word_embeddings_class'] = nn.Embedding\n",
    "        config['entity_embeddings_class'] = nn.Embedding\n",
    "        super(LocalCtxAttRanker, self).__init__(config)\n",
    "\n",
    "        self.hid_dims = config['hid_dims']\n",
    "        self.tok_top_n = config['tok_top_n']\n",
    "        self.margin = config['margin']\n",
    "\n",
    "        self.att_mat_diag = nn.Parameter(torch.ones(self.emb_dims))\n",
    "        self.tok_score_mat_diag = nn.Parameter(torch.ones(self.emb_dims))\n",
    "        self.local_ctx_dr = nn.Dropout(p=0)\n",
    "\n",
    "        self.score_combine_linear_1 = nn.Linear(2, self.hid_dims)\n",
    "        self.score_combine_act_1 = nn.ReLU()\n",
    "        self.score_combine_linear_2 = nn.Linear(self.hid_dims, 1)\n",
    "\n",
    "    def print_weight_norm(self):\n",
    "        print('att_mat_diag', self.att_mat_diag.data.norm())\n",
    "        print('tok_score_mat_diag', self.tok_score_mat_diag.data.norm())\n",
    "        print('f - l1.w, b', self.score_combine_linear_1.weight.data.norm(),  self.score_combine_linear_1.bias.data.norm())\n",
    "        print('f - l2.w, b', self.score_combine_linear_2.weight.data.norm(),  self.score_combine_linear_2.bias.data.norm())\n",
    "\n",
    "    def print_attention(self, gold_pos):\n",
    "        token_ids = self._token_ids.data.cpu().numpy()\n",
    "        entity_ids = self._entity_ids.data.cpu().numpy()\n",
    "        att_probs = self._att_probs.data.cpu().numpy()\n",
    "        top_tok_att_ids = self._top_tok_att_ids.data.cpu().numpy()\n",
    "        gold_pos = gold_pos.data.cpu().numpy()\n",
    "        scores = self._scores.data.cpu().numpy()\n",
    "\n",
    "        print('===========================================')\n",
    "        for tids, eids, ap, aids, gpos, ss in zip(token_ids, entity_ids, att_probs, top_tok_att_ids, gold_pos, scores):\n",
    "            selected_tids = tids[aids]\n",
    "            print('-------------------------------')\n",
    "            print(tokgreen(repr([(self.entity_voca.id2word[e], s) for e, s in zip(eids, ss)])),\n",
    "                  tokblue(repr(self.entity_voca.id2word[eids[gpos]] if gpos > -1 else 'UNKNOWN')))\n",
    "            print([(self.word_voca.id2word[t], a[0]) for t, a in zip(selected_tids, ap)])\n",
    "\n",
    "    def forward(self, token_ids, tok_mask, entity_ids, entity_mask, p_e_m=None):\n",
    "        batchsize, n_words = token_ids.size()\n",
    "        n_entities = entity_ids.size(1)\n",
    "        tok_mask = tok_mask.view(batchsize, 1, -1)\n",
    "\n",
    "        tok_vecs = self.word_embeddings(token_ids)\n",
    "        entity_vecs = self.entity_embeddings(entity_ids)\n",
    "\n",
    "        # att\n",
    "        ent_tok_att_scores = torch.bmm(entity_vecs * self.att_mat_diag, tok_vecs.permute(0, 2, 1))\n",
    "        ent_tok_att_scores = (ent_tok_att_scores * tok_mask).add_((tok_mask - 1).mul_(1e10))\n",
    "        tok_att_scores, _ = torch.max(ent_tok_att_scores, dim=1)\n",
    "        top_tok_att_scores, top_tok_att_ids = torch.topk(tok_att_scores, dim=1, k=min(self.tok_top_n, n_words))\n",
    "        att_probs = F.softmax(top_tok_att_scores, dim=1).view(batchsize, -1, 1)\n",
    "        att_probs = att_probs / torch.sum(att_probs, dim=1, keepdim=True)\n",
    "\n",
    "        selected_tok_vecs = torch.gather(tok_vecs, dim=1,\n",
    "                                         index=top_tok_att_ids.view(batchsize, -1, 1).repeat(1, 1, tok_vecs.size(2)))\n",
    "        ctx_vecs = torch.sum((selected_tok_vecs * self.tok_score_mat_diag) * att_probs, dim=1, keepdim=True)\n",
    "        ctx_vecs = self.local_ctx_dr(ctx_vecs)\n",
    "        ent_ctx_scores = torch.bmm(entity_vecs, ctx_vecs.permute(0, 2, 1)).view(batchsize, n_entities)\n",
    "\n",
    "        # combine with p(e|m) if p_e_m is not None\n",
    "        if p_e_m is not None:\n",
    "            inputs = torch.cat([ent_ctx_scores.view(batchsize * n_entities, -1),\n",
    "                                torch.log(p_e_m + 1e-20).view(batchsize * n_entities, -1)], dim=1)\n",
    "            hidden = self.score_combine_linear_1(inputs)\n",
    "            hidden = self.score_combine_act_1(hidden)\n",
    "            scores = self.score_combine_linear_2(hidden).view(batchsize, n_entities)\n",
    "        else:\n",
    "            scores = ent_ctx_scores\n",
    "\n",
    "        scores = (scores * entity_mask).add_((entity_mask - 1).mul_(1e10))\n",
    "\n",
    "        # printing attention (debugging)\n",
    "        self._token_ids = token_ids\n",
    "        self._entity_ids = entity_ids\n",
    "        self._att_probs = att_probs\n",
    "        self._top_tok_att_ids = top_tok_att_ids\n",
    "        self._scores = scores\n",
    "\n",
    "        self._entity_vecs = entity_vecs\n",
    "        self._local_ctx_vecs = ctx_vecs\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def regularize(self, max_norm=1):\n",
    "        l1_w_norm = self.score_combine_linear_1.weight.norm()\n",
    "        l1_b_norm = self.score_combine_linear_1.bias.norm()\n",
    "        l2_w_norm = self.score_combine_linear_2.weight.norm()\n",
    "        l2_b_norm = self.score_combine_linear_2.bias.norm()\n",
    "\n",
    "        if (l1_w_norm > max_norm).data.all():\n",
    "            self.score_combine_linear_1.weight.data = self.score_combine_linear_1.weight.data * max_norm / l1_w_norm.data\n",
    "        if (l1_b_norm > max_norm).data.all():\n",
    "            self.score_combine_linear_1.bias.data = self.score_combine_linear_1.bias.data *  max_norm / l1_b_norm.data\n",
    "        if (l2_w_norm > max_norm).data.all():\n",
    "            self.score_combine_linear_2.weight.data = self.score_combine_linear_2.weight.data * max_norm / l2_w_norm.data\n",
    "        if (l2_b_norm > max_norm).data.all():\n",
    "            self.score_combine_linear_2.bias.data = self.score_combine_linear_2.bias.data *  max_norm / l2_b_norm.data\n",
    "\n",
    "    def loss(self, scores, true_pos):\n",
    "        loss = F.multi_margin_loss(scores, true_pos, margin=self.margin)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eb90708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "totaltime = {}\n",
    "start_at = {}\n",
    "\n",
    "\n",
    "def tik(name):\n",
    "    start_at[name] = int(round(time.time() * 1000))\n",
    "\n",
    "\n",
    "def tok(name):\n",
    "    if name not in start_at:\n",
    "        raise Exception(\"not tik yet\")\n",
    "    if name not in totaltime:\n",
    "        totaltime[name] = 0.\n",
    "    totaltime[name] += int(round(time.time() * 1000)) - start_at[name]\n",
    "\n",
    "\n",
    "def print_time(name=None):\n",
    "    print('------- running time -------')\n",
    "    if name is not None:\n",
    "        print(name, totaltime[name])\n",
    "    else:\n",
    "        for name,t in totaltime.items():\n",
    "            print('---', name, t)\n",
    "    print('---------------------------')\n",
    "\n",
    "\n",
    "def reset():\n",
    "    global totaltime\n",
    "    global start_at\n",
    "    totaltime = {}\n",
    "    start_at = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ecd81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "datadir = 'data/generated/test_train_data/'\n",
    "conll_path = 'data/basic_data/test_datasets/'\n",
    "person_path = 'data/basic_data/p_e_m_data/persons.txt'\n",
    "voca_emb_dir = 'data/generated/embeddings/word_ent_embs/'\n",
    "\n",
    "ModelClass = MulRelRanker\n",
    "\n",
    "\n",
    "# general args\n",
    "parser.add_argument(\"--mode\", type=str,\n",
    "                    help=\"train or eval\",\n",
    "                    default='train')\n",
    "parser.add_argument(\"--model_path\", type=str,\n",
    "                    help=\"model path to save/load\",\n",
    "                    default='')\n",
    "\n",
    "# args for preranking (i.e. 2-step candidate selection)\n",
    "parser.add_argument(\"--n_cands_before_rank\", type=int,\n",
    "                    help=\"number of candidates\",\n",
    "                    default=30)\n",
    "parser.add_argument(\"--prerank_ctx_window\", type=int,\n",
    "                    help=\"size of context window for the preranking model\",\n",
    "                    default=50)\n",
    "parser.add_argument(\"--keep_p_e_m\", type=int,\n",
    "                    help=\"number of top candidates to keep w.r.t p(e|m)\",\n",
    "                    default=4)\n",
    "parser.add_argument(\"--keep_ctx_ent\", type=int,\n",
    "                    help=\"number of top candidates to keep w.r.t using context\",\n",
    "                    default=4)\n",
    "\n",
    "# args for local model\n",
    "parser.add_argument(\"--ctx_window\", type=int,\n",
    "                    help=\"size of context window for the local model\",\n",
    "                    default=100)\n",
    "parser.add_argument(\"--tok_top_n\", type=int,\n",
    "                    help=\"number of top contextual words for the local model\",\n",
    "                    default=25)\n",
    "\n",
    "\n",
    "# args for global model\n",
    "parser.add_argument(\"--mulrel_type\", type=str,\n",
    "                    help=\"type for multi relation (rel-norm or ment-norm)\",\n",
    "                    default='ment-norm')\n",
    "parser.add_argument(\"--n_rels\", type=int,\n",
    "                    help=\"number of relations\",\n",
    "                    default=5)\n",
    "parser.add_argument(\"--hid_dims\", type=int,\n",
    "                    help=\"number of hidden neurons\",\n",
    "                    default=100)\n",
    "parser.add_argument(\"--snd_local_ctx_window\", type=int,\n",
    "                    help=\"local ctx window size for relation scores\",\n",
    "                    default=6)\n",
    "parser.add_argument(\"--dropout_rate\", type=float,\n",
    "                    help=\"dropout rate for relation scores\",\n",
    "                    default=0.3)\n",
    "\n",
    "\n",
    "# args for training\n",
    "parser.add_argument(\"--n_epochs\", type=int,\n",
    "                    help=\"max number of epochs\",\n",
    "                    default=200)\n",
    "parser.add_argument(\"--dev_f1_change_lr\", type=float,\n",
    "                    help=\"dev f1 to change learning rate\",\n",
    "                    default=0.915)\n",
    "parser.add_argument(\"--n_not_inc\", type=int,\n",
    "                    help=\"number of evals after dev f1 not increase\",\n",
    "                    default=10)\n",
    "parser.add_argument(\"--eval_after_n_epochs\", type=int,\n",
    "                    help=\"number of epochs to eval\",\n",
    "                    default=5)\n",
    "parser.add_argument(\"--learning_rate\", type=float,\n",
    "                    help=\"learning rate\",\n",
    "                    default=1e-4)\n",
    "parser.add_argument(\"--margin\", type=float,\n",
    "                    help=\"margin\",\n",
    "                    default=0.01)\n",
    "\n",
    "# args for LBP\n",
    "parser.add_argument(\"--df\", type=float,\n",
    "                    help=\"dumpling factor (for LBP)\",\n",
    "                    default=0.5)\n",
    "parser.add_argument(\"--n_loops\", type=int,\n",
    "                    help=\"number of LBP loops\",\n",
    "                    default=10)\n",
    "\n",
    "# args for debugging\n",
    "parser.add_argument(\"--print_rel\", action='store_true')\n",
    "parser.add_argument(\"--print_incorrect\", action='store_true')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7d89644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load conll at data/generated/test_train_data/\n",
      "load csv\n",
      "process coref\n",
      "load conll\n",
      "create model\n",
      "word voca size 492408\n",
      "snd word voca size 60862\n",
      "{'args': Namespace(mode='train', model_path='', n_cands_before_rank=30, prerank_ctx_window=50, keep_p_e_m=4, keep_ctx_ent=4, ctx_window=100, tok_top_n=25, mulrel_type='ment-norm', n_rels=5, hid_dims=100, snd_local_ctx_window=6, dropout_rate=0.3, n_epochs=200, dev_f1_change_lr=0.915, n_not_inc=10, eval_after_n_epochs=5, learning_rate=0.0001, margin=0.01, df=0.5, n_loops=10, print_rel=False, print_incorrect=False),\n",
      " 'df': 0.5,\n",
      " 'dr': 0.3,\n",
      " 'emb_dims': 300,\n",
      " 'entity_embeddings': array([[ 0.06      , -0.075     ,  0.014     , ...,  0.083     ,\n",
      "        -0.02      , -0.031     ],\n",
      "       [-0.1       ,  0.058     ,  0.041     , ..., -0.075     ,\n",
      "         0.089     , -0.047     ],\n",
      "       [ 0.014     ,  0.062     ,  0.028     , ..., -0.026     ,\n",
      "         0.06      , -0.068     ],\n",
      "       ...,\n",
      "       [ 0.014     ,  0.018     , -0.025     , ...,  0.019     ,\n",
      "        -0.043     , -0.057     ],\n",
      "       [ 0.001     , -0.044     ,  0.149     , ...,  0.037     ,\n",
      "         0.005     ,  0.02      ],\n",
      "       [-0.00758579, -0.00171364,  0.03234197, ...,  0.01505074,\n",
      "        -0.02316106, -0.04260057]]),\n",
      " 'entity_voca': <__main__.Vocabulary object at 0x0000021B99E40460>,\n",
      " 'freeze_embs': True,\n",
      " 'hid_dims': 100,\n",
      " 'margin': 0.01,\n",
      " 'mulrel_type': 'ment-norm',\n",
      " 'n_loops': 10,\n",
      " 'n_rels': 5,\n",
      " 'snd_word_embeddings': array([[ 0.2587    ,  0.23414   ,  0.17771   , ..., -0.42938   ,\n",
      "        -0.48392   ,  0.60828   ],\n",
      "       [ 0.15139   , -0.20729   , -0.3755    , ...,  0.1377    ,\n",
      "         0.075059  , -0.27191   ],\n",
      "       [ 0.25826   ,  0.23759   ,  0.21491   , ...,  0.6305    ,\n",
      "         0.04818   , -0.45594   ],\n",
      "       ...,\n",
      "       [ 0.82401   , -0.15707   ,  0.29586   , ...,  0.44211   ,\n",
      "        -0.17721   ,  0.13085   ],\n",
      "       [ 1.1634    , -1.1471    , -0.53949   , ...,  0.41932   ,\n",
      "         0.99353   , -0.59911   ],\n",
      "       [ 0.22418612, -0.28881808,  0.13854355, ...,  0.19310516,\n",
      "        -0.07767657, -0.14481587]]),\n",
      " 'snd_word_voca': <__main__.Vocabulary object at 0x0000021B99E42B60>,\n",
      " 'tok_top_n': 25,\n",
      " 'word_embeddings': array([[ 8.00780000e-02,  1.04980000e-01,  4.98050000e-02, ...,\n",
      "         3.66200000e-03,  4.76070000e-02, -6.88480000e-02],\n",
      "       [ 7.03120000e-02,  8.69140000e-02,  8.78910000e-02, ...,\n",
      "        -4.76070000e-02,  1.44650000e-02, -6.25000000e-02],\n",
      "       [ 2.60010000e-02, -1.89200000e-03,  1.85547000e-01, ...,\n",
      "        -1.21582000e-01,  2.21680000e-01, -2.19730000e-02],\n",
      "       ...,\n",
      "       [ 2.81250000e-01, -2.13620000e-02,  1.72120000e-02, ...,\n",
      "         4.02830000e-02,  2.14844000e-01,  2.00195000e-01],\n",
      "       [ 1.86770000e-02,  1.28906000e-01,  5.17580000e-02, ...,\n",
      "         2.25830000e-02,  2.15820000e-01,  3.36910000e-02],\n",
      "       [-2.25727598e-04, -1.01302859e-03, -1.04770562e-02, ...,\n",
      "        -2.76348449e-02,  6.10916865e-02,  3.80460705e-02]]),\n",
      " 'word_voca': <__main__.Vocabulary object at 0x0000021B8EBABAC0>}\n",
      "--- create model ---\n",
      "prerank model\n",
      "main model\n",
      "create new model\n",
      "---------------- model config -----------------\n",
      "training True\n",
      "_non_persistent_buffers_set set()\n",
      "_is_full_backward_hook None\n",
      "emb_dims 300\n",
      "freeze_embs True\n",
      "hid_dims 100\n",
      "tok_top_n 25\n",
      "margin 0.01\n",
      "df 0.5\n",
      "n_loops 10\n",
      "n_rels 5\n",
      "dr 0.3\n",
      "ew_hid_dims 300\n",
      "max_dist 1000\n",
      "ent_top_n 1000\n",
      "oracle False\n",
      "ent_ent_comp bilinear\n",
      "ctx_comp bow\n",
      "mode ment-norm\n",
      "first_head_uniform False\n",
      "use_pad_ent True\n",
      "use_stargmax False\n",
      "use_local True\n",
      "use_local_only False\n",
      "freeze_local False\n",
      "_coh_ctx_vecs None\n",
      "-----------------------------------------------\n",
      "training...\n",
      "{'lr': 0.0001, 'n_epochs': 200}\n",
      "extracting training data\n",
      "108\n",
      "222\n",
      "288\n",
      "112\n",
      "182\n",
      "211\n",
      "105\n",
      "105\n",
      "recall 1.0\n",
      "#train docs 953\n",
      "277\n",
      "recall 0.9730745147150908\n",
      "aida-A #dev docs 218\n",
      "108\n",
      "114\n",
      "recall 0.9828316610925306\n",
      "aida-B #dev docs 232\n",
      "recall 0.9847560975609756\n",
      "msnbc #dev docs 20\n",
      "recall 0.9436038514442916\n",
      "aquaint #dev docs 50\n",
      "recall 0.9066147859922179\n",
      "ace2004 #dev docs 35\n",
      "recall 0.9169804554419939\n",
      "clueweb #dev docs 320\n",
      "recall 0.923418095801301\n",
      "wikipedia #dev docs 318\n",
      "creating optimizer\n",
      "epoch 0 0.52% -3.1622776e-09\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [5, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [17, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [7, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [6, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [32, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [9, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [11, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [23, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [18, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [1, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [61, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [8, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [10, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 0.63% 0.009599868\r",
      "epoch 0 0.73% 0.030653555\r",
      "epoch 0 0.84% -3.1622776e-09\r",
      "epoch 0 0.94% -3.1622776e-09\r",
      "epoch 0 1.05% -3.1622776e-09\r",
      "epoch 0 1.15% 0.0090118535\r",
      "epoch 0 1.26% 0.010226317\r",
      "epoch 0 1.36% -3.1622776e-09\r",
      "epoch 0 1.47% 0.011400476\r",
      "epoch 0 1.57% 0.01960607\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [29, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [27, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [36, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [12, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [100, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 3.88% 0.003629722409\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [15, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [69, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [22, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [26, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [56, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [37, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [4, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [58, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [62, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [60, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [43, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [41, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [3, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [19, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 6.82% 0.017629059049\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [25, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [38, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [28, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [2, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [30, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [13, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [33, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 6.93% 0.015093019\r",
      "epoch 0 7.03% 0.0060781203\r",
      "epoch 0 7.14% 0.0023291365\r",
      "epoch 0 7.24% 0.002380739\r",
      "epoch 0 7.35% -3.1622776e-09\r",
      "epoch 0 7.45% -3.1622776e-09\r",
      "epoch 0 7.56% 0.009906992\r",
      "epoch 0 7.66% -3.1622776e-09\r",
      "epoch 0 7.76% 0.0021020146\r",
      "epoch 0 7.87% -3.1622776e-09\r",
      "epoch 0 7.97% -3.1622776e-09\r",
      "epoch 0 8.08% 0.0017806442\r",
      "epoch 0 8.18% 0.0014314814\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [52, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [21, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [47, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 10.91% -3.1622776e-09\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [67, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [80, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 11.02% 0.00017395771\r",
      "epoch 0 11.12% -3.1622776e-09\r",
      "epoch 0 11.23% -3.1622776e-09\r",
      "epoch 0 11.33% 0.03140534\r",
      "epoch 0 11.44% 0.001059558\r",
      "epoch 0 11.54% 0.0013248508\r",
      "epoch 0 11.65% 0.00068837014\r",
      "epoch 0 11.75% 0.03612699\r",
      "epoch 0 11.86% 0.0003128293\r",
      "epoch 0 11.96% 0.0012280152\r",
      "epoch 0 12.07% 0.006698726\r",
      "epoch 0 12.17% -3.1622776e-09\r",
      "epoch 0 12.28% -3.1622776e-09\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [16, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [59, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [14, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 12.38% 0.0005999954\r",
      "epoch 0 12.49% 0.015689798\r",
      "epoch 0 12.59% 0.019660551\r",
      "epoch 0 12.70% 0.0799461\r",
      "epoch 0 12.80% -3.1622776e-09\r",
      "epoch 0 12.91% 0.02307387\r",
      "epoch 0 13.01% 0.001462177\r",
      "epoch 0 13.12% 0.030512244\r",
      "epoch 0 13.22% -3.1622776e-09\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [39, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [31, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 16.79% 0.002718228609\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [35, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 16.89% 0.014491152\r",
      "epoch 0 17.00% 0.0036770222\r",
      "epoch 0 17.10% 7.5256845e-05\r",
      "epoch 0 17.21% 0.00042263296\r",
      "epoch 0 17.31% -3.1622776e-09\r",
      "epoch 0 17.42% 0.0010779361\r",
      "epoch 0 17.52% 0.0011833672\r",
      "epoch 0 17.63% 0.005364581\r",
      "epoch 0 17.73% 0.007966773\r",
      "epoch 0 17.84% -3.1622776e-09\r",
      "epoch 0 17.94% 0.0085604265\r",
      "epoch 0 18.05% 0.005338028\r",
      "epoch 0 18.15% 0.0018571042\r",
      "epoch 0 18.26% 0.007395204\r",
      "epoch 0 18.36% 0.0006858181\r",
      "epoch 0 18.47% 0.00096230523\r",
      "epoch 0 18.57% 0.006143187\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [44, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 21.93% 0.003088459879\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [40, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [24, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [90, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 24.24% 0.001858393859\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [77, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 24.34% 0.0028254683\r",
      "epoch 0 24.45% 0.00025767146\r",
      "epoch 0 24.55% 0.0018945585\r",
      "epoch 0 24.66% 0.009097236\r",
      "epoch 0 24.76% 0.0038019507\r",
      "epoch 0 24.87% 0.007903037\r",
      "epoch 0 24.97% -3.1622776e-09\r",
      "epoch 0 25.08% 0.0024352176\r",
      "epoch 0 25.18% 0.0017438522\r",
      "epoch 0 25.29% 0.002868364\r",
      "epoch 0 25.39% 0.00075482274\r",
      "epoch 0 25.50% 0.00030977637\r",
      "epoch 0 25.60% 0.0002740437\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [95, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [20, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 25.71% -3.1622776e-09\r",
      "epoch 0 25.81% -3.1622776e-09\r",
      "epoch 0 25.92% -3.1622776e-09\r",
      "epoch 0 26.02% 0.0033034747\r",
      "epoch 0 26.13% 0.0001305126\r",
      "epoch 0 26.23% 0.007344491\r",
      "epoch 0 26.34% 0.006622865\r",
      "epoch 0 26.44% 0.0007318565\r",
      "epoch 0 26.55% 0.006056784\r",
      "epoch 0 26.65% 0.0009135384\r",
      "epoch 0 26.76% 0.0006879401\r",
      "epoch 0 26.86% 0.006352129\r",
      "epoch 0 26.97% 0.003747736\r",
      "epoch 0 27.07% 0.0018585352\r",
      "epoch 0 27.18% 0.00030010176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [99, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [48, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 27.28% 0.00014817672\r",
      "epoch 0 27.39% 0.0028833249\r",
      "epoch 0 27.49% 0.0014763328\r",
      "epoch 0 27.60% 0.0022900258\r",
      "epoch 0 27.70% 0.0018417381\r",
      "epoch 0 27.81% 0.0010929675\r",
      "epoch 0 27.91% 0.0036643317\r",
      "epoch 0 28.02% -3.1622776e-09\r",
      "epoch 0 28.12% 0.0021401579\r",
      "epoch 0 28.23% -3.1622776e-09\r",
      "epoch 0 28.33% 0.0046823435\r",
      "epoch 0 28.44% -3.1622776e-09\r",
      "epoch 0 28.54% 0.0018657987\r",
      "epoch 0 28.65% 9.85078e-05\r",
      "epoch 0 28.75% 0.0006174272\r",
      "epoch 0 28.86% 0.0023694762\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [34, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 35.05% 0.001964189359\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [74, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 38.09% 0.005838713177\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [82, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [42, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 44.18% 0.000889902446\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [57, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 44.28% 0.0018033917\r",
      "epoch 0 44.39% -1.1030456e-06\r",
      "epoch 0 44.49% 0.0002222654\r",
      "epoch 0 44.60% 0.00021747801\r",
      "epoch 0 44.70% 0.0009104105\r",
      "epoch 0 44.81% 0.0017208899\r",
      "epoch 0 44.91% 0.000648185\r",
      "epoch 0 45.02% -1.101469e-06\r",
      "epoch 0 45.12% 0.0012527276\r",
      "epoch 0 45.23% 0.0015180184\r",
      "epoch 0 45.33% -1.1013333e-06\r",
      "epoch 0 45.44% 0.0011546082\r",
      "epoch 0 45.54% -1.1013256e-06\r",
      "epoch 0 45.65% -1.1013847e-06\r",
      "epoch 0 45.75% 0.0012679126\r",
      "epoch 0 45.86% -1.1016201e-06\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [64, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 49.00% 0.000136833176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [68, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 52.47% 0.004899784836\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [49, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 61.59% -1.8426231e-06\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [63, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 67.79% 0.000154406666\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [45, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 72.30% 0.001382788506\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [81, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [51, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 82.79% 3.257003e-0556\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [65, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 85.41% 0.000547564606\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [70, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [54, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 89.40% 0.003921978676\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [53, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 89.51% 0.0020178678\r",
      "epoch 0 89.61% -1.5907999e-06\r",
      "epoch 0 89.72% 0.00025309564\r",
      "epoch 0 89.82% 0.0025684347\r",
      "epoch 0 89.93% 0.0004264762\r",
      "epoch 0 90.03% -1.5856754e-06\r",
      "epoch 0 90.14% 0.0006410343\r",
      "epoch 0 90.24% 0.00044036497\r",
      "epoch 0 90.35% 0.0031858848\r",
      "epoch 0 90.45% 0.003083799\r",
      "epoch 0 90.56% 0.00035074138\r",
      "epoch 0 90.66% 0.00011732842\r",
      "epoch 0 90.77% 0.0005510752\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ufukh\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [88, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:33.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total loss 3.2029768098186224 0.0033609410386344412\n",
      "epoch 1 total loss 0.8445715113322194 0.0008862240412720036\n",
      "epoch 2 total loss 0.6542167932067287 0.0006864814199441014\n",
      "epoch 3 total loss 0.5834956527050963 0.0006122724582424935\n",
      "epoch 4 total loss 0.49960529303098156 0.0005242447985634644\n",
      "aida-A \u001b[92mmicro F1: 0.8364471349545977\u001b[0m\n",
      "aida-B \u001b[92mmicro F1: 0.8234113712374583\u001b[0m\n",
      "msnbc \u001b[92mmicro F1: 0.9349655700076511\u001b[0m\n",
      "aquaint \u001b[92mmicro F1: 0.8881118881118881\u001b[0m\n",
      "ace2004 \u001b[92mmicro F1: 0.8853118712273641\u001b[0m\n",
      "clueweb \u001b[92mmicro F1: 0.7614983830398849\u001b[0m\n",
      "wikipedia \u001b[92mmicro F1: 0.7357443976037276\u001b[0m\n",
      "att_mat_diag tensor(17.4242, device='cuda:0')\n",
      "tok_score_mat_diag tensor(17.8641, device='cuda:0')\n",
      "f - l1.w, b tensor(5.3760, device='cuda:0') tensor(4.0614, device='cuda:0')\n",
      "f - l2.w, b tensor(0.5742, device='cuda:0') tensor(0.0088, device='cuda:0')\n",
      "tensor(13.1788, device='cuda:0') tensor(1.0102, device='cuda:0')\n",
      "relations tensor([18.8949,  1.6499,  1.6459,  1.6533,  1.6140], device='cuda:0')\n",
      "tensor([[0.0000, 0.4691, 0.4709, 0.4703, 0.4438],\n",
      "        [0.4691, 0.0000, 0.3266, 0.3163, 0.3245],\n",
      "        [0.4709, 0.3266, 0.0000, 0.3241, 0.3193],\n",
      "        [0.4703, 0.3163, 0.3241, 0.0000, 0.3207],\n",
      "        [0.4438, 0.3245, 0.3193, 0.3207, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "ew_embs tensor([1.9878, 1.4850, 1.4484, 1.4444, 1.4684], device='cuda:0')\n",
      "tensor([[0.0000, 0.9514, 0.9537, 0.9587, 0.9582],\n",
      "        [0.9514, 0.0000, 1.0003, 1.0005, 1.0004],\n",
      "        [0.9537, 1.0003, 0.0000, 1.0006, 1.0006],\n",
      "        [0.9587, 1.0005, 1.0006, 0.0000, 1.0004],\n",
      "        [0.9582, 1.0004, 1.0006, 1.0004, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "epoch 5 total loss 0.4736831332137399 0.0004970442111371877\n",
      "epoch 6 total loss 0.39919391534846227 0.00041888133824602544\n",
      "epoch 7 total loss 0.3996716551767463 0.00041938263922009053\n",
      "epoch 8 total loss 0.39045209910023004 0.00040970839359940193\n",
      "epoch 9 total loss 0.380245425334067 0.0003989983476747818\n",
      "aida-A \u001b[92mmicro F1: 0.8882162613505896\u001b[0m\n",
      "aida-B \u001b[92mmicro F1: 0.9018952062430323\u001b[0m\n",
      "msnbc \u001b[92mmicro F1: 0.9410864575363428\u001b[0m\n",
      "aquaint \u001b[92mmicro F1: 0.8811188811188811\u001b[0m\n",
      "ace2004 \u001b[92mmicro F1: 0.8812877263581488\u001b[0m\n",
      "clueweb \u001b[92mmicro F1: 0.7689543657923105\u001b[0m\n",
      "wikipedia \u001b[92mmicro F1: 0.7620738111086458\u001b[0m\n",
      "att_mat_diag tensor(17.4506, device='cuda:0')\n",
      "tok_score_mat_diag tensor(17.9035, device='cuda:0')\n",
      "f - l1.w, b tensor(5.3760, device='cuda:0') tensor(4.0614, device='cuda:0')\n",
      "f - l2.w, b tensor(0.5742, device='cuda:0') tensor(0.0088, device='cuda:0')\n",
      "tensor(15.9828, device='cuda:0') tensor(1.1142, device='cuda:0')\n",
      "relations tensor([19.3969,  2.2505,  2.2427,  2.2482,  2.2094], device='cuda:0')\n",
      "tensor([[0.0000, 0.5168, 0.5178, 0.5157, 0.4982],\n",
      "        [0.5168, 0.0000, 0.2738, 0.2632, 0.2711],\n",
      "        [0.5178, 0.2738, 0.0000, 0.2707, 0.2658],\n",
      "        [0.5157, 0.2632, 0.2707, 0.0000, 0.2676],\n",
      "        [0.4982, 0.2711, 0.2658, 0.2676, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "ew_embs tensor([3.0844, 2.4884, 2.4366, 2.4095, 2.4725], device='cuda:0')\n",
      "tensor([[0.0000, 0.7730, 0.7915, 0.8033, 0.8069],\n",
      "        [0.7730, 0.0000, 0.9839, 0.9904, 0.9891],\n",
      "        [0.7915, 0.9839, 0.0000, 0.9886, 0.9923],\n",
      "        [0.8033, 0.9904, 0.9886, 0.0000, 0.9889],\n",
      "        [0.8069, 0.9891, 0.9923, 0.9889, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "epoch 10 total loss 0.3710072986659725 0.00038930461559913166\n",
      "epoch 11 total loss 0.35836730156870544 0.0003760412398412439\n",
      "epoch 12 total loss 0.355822424413077 0.000373370854578255\n",
      "epoch 13 total loss 0.34209188554837056 0.0003589631537758348\n",
      "epoch 14 total loss 0.3190675873405553 0.0003348033445336362\n",
      "aida-A \u001b[92mmicro F1: 0.8994885711303621\u001b[0m\n",
      "aida-B \u001b[92mmicro F1: 0.9072463768115943\u001b[0m\n",
      "msnbc \u001b[92mmicro F1: 0.9319051262433052\u001b[0m\n",
      "aquaint \u001b[92mmicro F1: 0.8979020979020979\u001b[0m\n",
      "ace2004 \u001b[92mmicro F1: 0.9014084507042254\u001b[0m\n",
      "clueweb \u001b[92mmicro F1: 0.770212001437298\u001b[0m\n",
      "wikipedia \u001b[92mmicro F1: 0.7798239775164558\u001b[0m\n",
      "att_mat_diag tensor(17.4474, device='cuda:0')\n",
      "tok_score_mat_diag tensor(17.8616, device='cuda:0')\n",
      "f - l1.w, b tensor(5.3760, device='cuda:0') tensor(4.0614, device='cuda:0')\n",
      "f - l2.w, b tensor(0.5742, device='cuda:0') tensor(0.0088, device='cuda:0')\n",
      "tensor(18.6705, device='cuda:0') tensor(1.1952, device='cuda:0')\n",
      "relations tensor([19.6765,  2.6113,  2.5973,  2.5996,  2.5616], device='cuda:0')\n",
      "tensor([[0.0000, 0.5606, 0.5627, 0.5596, 0.5464],\n",
      "        [0.5606, 0.0000, 0.2589, 0.2478, 0.2559],\n",
      "        [0.5627, 0.2589, 0.0000, 0.2556, 0.2506],\n",
      "        [0.5596, 0.2478, 0.2556, 0.0000, 0.2528],\n",
      "        [0.5464, 0.2559, 0.2506, 0.2528, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "ew_embs tensor([4.0560, 3.3664, 3.3029, 3.2437, 3.3325], device='cuda:0')\n",
      "tensor([[0.0000, 0.6422, 0.6598, 0.6758, 0.6760],\n",
      "        [0.6422, 0.0000, 0.8740, 0.8825, 0.8754],\n",
      "        [0.6598, 0.8740, 0.0000, 0.8847, 0.8925],\n",
      "        [0.6758, 0.8825, 0.8847, 0.0000, 0.8911],\n",
      "        [0.6760, 0.8754, 0.8925, 0.8911, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "epoch 15 total loss 0.3364142913550836 0.0003530055523138338\n",
      "epoch 16 total loss 0.30967014526277126 0.00032494243993994883\n",
      "epoch 17 total loss 0.30863958961316484 0.0003238610594052097\n",
      "epoch 18 total loss 0.296344941384973 0.00031096006441235364\n",
      "epoch 19 total loss 0.31777193510947654 0.0003334437933992409\n",
      "aida-A \u001b[92mmicro F1: 0.8675503600876735\u001b[0m\n",
      "aida-B \u001b[92mmicro F1: 0.8896321070234113\u001b[0m\n",
      "msnbc \u001b[92mmicro F1: 0.9273144605967865\u001b[0m\n",
      "aquaint \u001b[92mmicro F1: 0.8363636363636363\u001b[0m\n",
      "ace2004 \u001b[92mmicro F1: 0.8893360160965794\u001b[0m\n",
      "clueweb \u001b[92mmicro F1: 0.7399389148401007\u001b[0m\n",
      "wikipedia \u001b[92mmicro F1: 0.7647363360698174\u001b[0m\n",
      "att_mat_diag tensor(17.4366, device='cuda:0')\n",
      "tok_score_mat_diag tensor(17.7827, device='cuda:0')\n",
      "f - l1.w, b tensor(5.3760, device='cuda:0') tensor(4.0614, device='cuda:0')\n",
      "f - l2.w, b tensor(0.5742, device='cuda:0') tensor(0.0088, device='cuda:0')\n",
      "tensor(21.0037, device='cuda:0') tensor(1.2697, device='cuda:0')\n",
      "relations tensor([19.8916,  2.9042,  2.8835,  2.8801,  2.8471], device='cuda:0')\n",
      "tensor([[0.0000, 0.5976, 0.6016, 0.5983, 0.5888],\n",
      "        [0.5976, 0.0000, 0.2518, 0.2404, 0.2485],\n",
      "        [0.6016, 0.2518, 0.0000, 0.2486, 0.2432],\n",
      "        [0.5983, 0.2404, 0.2486, 0.0000, 0.2460],\n",
      "        [0.5888, 0.2485, 0.2432, 0.2460, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "ew_embs tensor([4.9459, 4.1703, 4.1032, 4.0157, 4.1248], device='cuda:0')\n",
      "tensor([[0.0000, 0.5619, 0.5799, 0.5952, 0.5940],\n",
      "        [0.5619, 0.0000, 0.7925, 0.7962, 0.7852],\n",
      "        [0.5799, 0.7925, 0.0000, 0.8035, 0.8097],\n",
      "        [0.5952, 0.7962, 0.8035, 0.0000, 0.8125],\n",
      "        [0.5940, 0.7852, 0.8097, 0.8125, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "epoch 20 total loss 0.3128663278540671 0.00032829625168317636\n",
      "epoch 21 total loss 0.27979692993540084 0.00029359593907177424\n",
      "epoch 22 total loss 0.2764524633646488 0.0002900865302881939\n",
      "epoch 23 total loss 0.2813039175302947 0.0002951772481954824\n",
      "epoch 24 total loss 0.2697659853468508 0.0002830702889263912\n",
      "aida-A \u001b[92mmicro F1: 0.9065859513620708\u001b[0m\n",
      "aida-B \u001b[92mmicro F1: 0.9181716833890747\u001b[0m\n",
      "msnbc \u001b[92mmicro F1: 0.938026013771997\u001b[0m\n",
      "aquaint \u001b[92mmicro F1: 0.890909090909091\u001b[0m\n",
      "ace2004 \u001b[92mmicro F1: 0.9054325955734406\u001b[0m\n",
      "clueweb \u001b[92mmicro F1: 0.7759611929572403\u001b[0m\n",
      "wikipedia \u001b[92mmicro F1: 0.777013534501886\u001b[0m\n",
      "att_mat_diag tensor(17.4274, device='cuda:0')\n",
      "tok_score_mat_diag tensor(17.6980, device='cuda:0')\n",
      "f - l1.w, b tensor(5.3760, device='cuda:0') tensor(4.0614, device='cuda:0')\n",
      "f - l2.w, b tensor(0.5742, device='cuda:0') tensor(0.0088, device='cuda:0')\n",
      "tensor(23.0310, device='cuda:0') tensor(1.3040, device='cuda:0')\n",
      "relations tensor([20.0657,  3.1509,  3.1222,  3.1157,  3.0877], device='cuda:0')\n",
      "tensor([[0.0000, 0.6267, 0.6335, 0.6303, 0.6236],\n",
      "        [0.6267, 0.0000, 0.2468, 0.2351, 0.2438],\n",
      "        [0.6335, 0.2468, 0.0000, 0.2437, 0.2380],\n",
      "        [0.6303, 0.2351, 0.2437, 0.0000, 0.2415],\n",
      "        [0.6236, 0.2438, 0.2380, 0.2415, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "ew_embs tensor([5.6984, 4.8620, 4.7894, 4.6834, 4.8130], device='cuda:0')\n",
      "tensor([[0.0000, 0.5101, 0.5289, 0.5417, 0.5423],\n",
      "        [0.5101, 0.0000, 0.7345, 0.7327, 0.7219],\n",
      "        [0.5289, 0.7345, 0.0000, 0.7451, 0.7486],\n",
      "        [0.5417, 0.7327, 0.7451, 0.0000, 0.7541],\n",
      "        [0.5423, 0.7219, 0.7486, 0.7541, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 total loss 0.2687396475818673 0.0002819933342936698\n",
      "epoch 26 total loss 0.26674645857889345 0.0002799018453083877\n",
      "epoch 27 total loss 0.2626942512661685 0.0002756497914650247\n",
      "epoch 28 total loss 0.2787444858000754 0.0002924915905562176\n",
      "epoch 29 total loss 0.2471340813709162 0.0002593222259925668\n",
      "aida-A \u001b[92mmicro F1: 0.8913474585116377\u001b[0m\n",
      "aida-B \u001b[92mmicro F1: 0.8925306577480491\u001b[0m\n",
      "msnbc \u001b[92mmicro F1: 0.9349655700076511\u001b[0m\n",
      "aquaint \u001b[92mmicro F1: 0.9034965034965036\u001b[0m\n",
      "ace2004 \u001b[92mmicro F1: 0.9014084507042254\u001b[0m\n",
      "clueweb \u001b[92mmicro F1: 0.775601868487244\u001b[0m\n",
      "wikipedia \u001b[92mmicro F1: 0.7645884180164189\u001b[0m\n",
      "att_mat_diag tensor(17.4062, device='cuda:0')\n",
      "tok_score_mat_diag tensor(17.6131, device='cuda:0')\n",
      "f - l1.w, b tensor(5.3760, device='cuda:0') tensor(4.0614, device='cuda:0')\n",
      "f - l2.w, b tensor(0.5742, device='cuda:0') tensor(0.0088, device='cuda:0')\n",
      "tensor(24.8261, device='cuda:0') tensor(1.3374, device='cuda:0')\n",
      "relations tensor([20.2224,  3.3582,  3.3225,  3.3127,  3.2893], device='cuda:0')\n",
      "tensor([[0.0000, 0.6428, 0.6519, 0.6489, 0.6439],\n",
      "        [0.6428, 0.0000, 0.2444, 0.2327, 0.2420],\n",
      "        [0.6519, 0.2444, 0.0000, 0.2415, 0.2355],\n",
      "        [0.6489, 0.2327, 0.2415, 0.0000, 0.2399],\n",
      "        [0.6439, 0.2420, 0.2355, 0.2399, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "ew_embs tensor([6.4026, 5.5411, 5.4533, 5.3400, 5.4805], device='cuda:0')\n",
      "tensor([[0.0000, 0.4738, 0.4957, 0.5057, 0.5076],\n",
      "        [0.4738, 0.0000, 0.6924, 0.6877, 0.6764],\n",
      "        [0.4957, 0.6924, 0.0000, 0.7030, 0.7062],\n",
      "        [0.5057, 0.6877, 0.7030, 0.0000, 0.7127],\n",
      "        [0.5076, 0.6764, 0.7062, 0.7127, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "epoch 30 total loss 0.24696430554109838 0.0002591440771679941\n",
      "epoch 31 total loss 0.23941880863264942 0.00025122645187056604\n",
      "epoch 32 total loss 0.2421143109184527 0.00025405489078536483\n",
      "epoch 33 total loss 0.22883336568156665 0.00024011895664382649\n",
      "epoch 34 total loss 0.22807379815481 0.00023932192880882477\n",
      "aida-A \u001b[92mmicro F1: 0.9001148105625717\u001b[0m\n",
      "aida-B \u001b[92mmicro F1: 0.9217391304347826\u001b[0m\n",
      "msnbc \u001b[92mmicro F1: 0.9349655700076511\u001b[0m\n",
      "aquaint \u001b[92mmicro F1: 0.8615384615384615\u001b[0m\n",
      "ace2004 \u001b[92mmicro F1: 0.8812877263581488\u001b[0m\n",
      "clueweb \u001b[92mmicro F1: 0.7551203736974488\u001b[0m\n",
      "wikipedia \u001b[92mmicro F1: 0.774203091487316\u001b[0m\n",
      "att_mat_diag tensor(17.3821, device='cuda:0')\n",
      "tok_score_mat_diag tensor(17.5116, device='cuda:0')\n",
      "f - l1.w, b tensor(5.3760, device='cuda:0') tensor(4.0614, device='cuda:0')\n",
      "f - l2.w, b tensor(0.5742, device='cuda:0') tensor(0.0088, device='cuda:0')\n",
      "tensor(26.5746, device='cuda:0') tensor(1.3513, device='cuda:0')\n",
      "relations tensor([20.3642,  3.5562,  3.5114,  3.4987,  3.4810], device='cuda:0')\n",
      "tensor([[0.0000, 0.6593, 0.6696, 0.6672, 0.6633],\n",
      "        [0.6593, 0.0000, 0.2443, 0.2321, 0.2420],\n",
      "        [0.6696, 0.2443, 0.0000, 0.2416, 0.2353],\n",
      "        [0.6672, 0.2321, 0.2416, 0.0000, 0.2404],\n",
      "        [0.6633, 0.2420, 0.2353, 0.2404, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "ew_embs tensor([7.1491, 6.2400, 6.1273, 6.0089, 6.1563], device='cuda:0')\n",
      "tensor([[0.0000, 0.4481, 0.4729, 0.4810, 0.4842],\n",
      "        [0.4481, 0.0000, 0.6627, 0.6572, 0.6453],\n",
      "        [0.4729, 0.6627, 0.0000, 0.6741, 0.6766],\n",
      "        [0.4810, 0.6572, 0.6741, 0.0000, 0.6834],\n",
      "        [0.4842, 0.6453, 0.6766, 0.6834, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "epoch 35 total loss 0.2364317807102907 0.0002480921098743869\n",
      "epoch 36 total loss 0.23581006956783312 0.0002474397372170337\n",
      "epoch 37 total loss 0.21801940774111017 0.0002287716765384157\n",
      "epoch 38 total loss 0.2216871405049119 0.00023262029433883724\n",
      "epoch 39 total loss 0.20993192380899472 0.00022028533453199867\n",
      "aida-A \u001b[92mmicro F1: 0.913265838638973\u001b[0m\n",
      "aida-B \u001b[92mmicro F1: 0.9290969899665552\u001b[0m\n",
      "msnbc \u001b[92mmicro F1: 0.9395562356541699\u001b[0m\n",
      "aquaint \u001b[92mmicro F1: 0.8797202797202797\u001b[0m\n",
      "ace2004 \u001b[92mmicro F1: 0.8772635814889336\u001b[0m\n",
      "clueweb \u001b[92mmicro F1: 0.7654509522098455\u001b[0m\n",
      "wikipedia \u001b[92mmicro F1: 0.7762739442348939\u001b[0m\n",
      "att_mat_diag tensor(17.3643, device='cuda:0')\n",
      "tok_score_mat_diag tensor(17.4275, device='cuda:0')\n",
      "f - l1.w, b tensor(5.3760, device='cuda:0') tensor(4.0614, device='cuda:0')\n",
      "f - l2.w, b tensor(0.5742, device='cuda:0') tensor(0.0088, device='cuda:0')\n",
      "tensor(28.2433, device='cuda:0') tensor(1.3673, device='cuda:0')\n",
      "relations tensor([20.5080,  3.7482,  3.6956,  3.6798,  3.6638], device='cuda:0')\n",
      "tensor([[0.0000, 0.6718, 0.6842, 0.6824, 0.6785],\n",
      "        [0.6718, 0.0000, 0.2443, 0.2317, 0.2422],\n",
      "        [0.6842, 0.2443, 0.0000, 0.2417, 0.2352],\n",
      "        [0.6824, 0.2317, 0.2417, 0.0000, 0.2408],\n",
      "        [0.6785, 0.2422, 0.2352, 0.2408, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "ew_embs tensor([7.8833, 6.9345, 6.7960, 6.6659, 6.8127], device='cuda:0')\n",
      "tensor([[0.0000, 0.4261, 0.4535, 0.4597, 0.4627],\n",
      "        [0.4261, 0.0000, 0.6355, 0.6294, 0.6171],\n",
      "        [0.4535, 0.6355, 0.0000, 0.6475, 0.6497],\n",
      "        [0.4597, 0.6294, 0.6475, 0.0000, 0.6572],\n",
      "        [0.4627, 0.6171, 0.6497, 0.6572, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "epoch 40 total loss 0.20649656687561446 0.00021668055286003617\n",
      "epoch 41 total loss 0.20117975942264366 0.0002111015313983669\n",
      "epoch 42 total loss 0.20061602940012335 0.00021050999937053866\n",
      "epoch 43 total loss 0.1919086573077493 0.00020137319759470023\n",
      "epoch 44 total loss 0.1968402865338703 0.0002065480446315533\n",
      "aida-A \u001b[92mmicro F1: 0.9149358104581986\u001b[0m\n",
      "aida-B \u001b[92mmicro F1: 0.9302118171683389\u001b[0m\n",
      "msnbc \u001b[92mmicro F1: 0.9395562356541699\u001b[0m\n",
      "aquaint \u001b[92mmicro F1: 0.8741258741258742\u001b[0m\n",
      "ace2004 \u001b[92mmicro F1: 0.8571428571428572\u001b[0m\n",
      "clueweb \u001b[92mmicro F1: 0.7657204455623428\u001b[0m\n",
      "wikipedia \u001b[92mmicro F1: 0.7781968789290732\u001b[0m\n",
      "att_mat_diag tensor(17.3505, device='cuda:0')\n",
      "tok_score_mat_diag tensor(17.3478, device='cuda:0')\n",
      "f - l1.w, b tensor(5.3760, device='cuda:0') tensor(4.0614, device='cuda:0')\n",
      "f - l2.w, b tensor(0.5742, device='cuda:0') tensor(0.0088, device='cuda:0')\n",
      "tensor(29.6890, device='cuda:0') tensor(1.3609, device='cuda:0')\n",
      "relations tensor([20.6304,  3.9116,  3.8495,  3.8330,  3.8179], device='cuda:0')\n",
      "tensor([[0.0000, 0.6832, 0.6971, 0.6963, 0.6923],\n",
      "        [0.6832, 0.0000, 0.2455, 0.2327, 0.2438],\n",
      "        [0.6971, 0.2455, 0.0000, 0.2433, 0.2363],\n",
      "        [0.6963, 0.2327, 0.2433, 0.0000, 0.2427],\n",
      "        [0.6923, 0.2438, 0.2363, 0.2427, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "ew_embs tensor([8.5927, 7.6140, 7.4578, 7.3142, 7.4631], device='cuda:0')\n",
      "tensor([[0.0000, 0.4064, 0.4346, 0.4402, 0.4434],\n",
      "        [0.4064, 0.0000, 0.6099, 0.6041, 0.5915],\n",
      "        [0.4346, 0.6099, 0.0000, 0.6229, 0.6248],\n",
      "        [0.4402, 0.6041, 0.6229, 0.0000, 0.6327],\n",
      "        [0.4434, 0.5915, 0.6248, 0.6327, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "epoch 45 total loss 0.18750954282404564 0.000196757127832157\n",
      "epoch 46 total loss 0.17730294411012437 0.00018604716066120082\n",
      "epoch 47 total loss 0.17190032607641115 0.00018037809661743036\n",
      "epoch 48 total loss 0.18575459097695557 0.0001949156253693133\n",
      "epoch 49 total loss 0.17318807697995453 0.00018172935674706667\n",
      "aida-A \u001b[92mmicro F1: 0.9084646696586994\u001b[0m\n",
      "aida-B \u001b[92mmicro F1: 0.9279821627647714\u001b[0m\n",
      "msnbc \u001b[92mmicro F1: 0.9257842387146136\u001b[0m\n",
      "aquaint \u001b[92mmicro F1: 0.8391608391608392\u001b[0m\n",
      "ace2004 \u001b[92mmicro F1: 0.841046277665996\u001b[0m\n",
      "clueweb \u001b[92mmicro F1: 0.7531440891124687\u001b[0m\n",
      "wikipedia \u001b[92mmicro F1: 0.7719843206863398\u001b[0m\n",
      "att_mat_diag tensor(17.3363, device='cuda:0')\n",
      "tok_score_mat_diag tensor(17.2578, device='cuda:0')\n",
      "f - l1.w, b tensor(5.3760, device='cuda:0') tensor(4.0614, device='cuda:0')\n",
      "f - l2.w, b tensor(0.5742, device='cuda:0') tensor(0.0088, device='cuda:0')\n",
      "tensor(31.0299, device='cuda:0') tensor(1.3557, device='cuda:0')\n",
      "relations tensor([20.7562,  4.0719,  4.0002,  3.9819,  3.9662], device='cuda:0')\n",
      "tensor([[0.0000, 0.6919, 0.7075, 0.7069, 0.7030],\n",
      "        [0.6919, 0.0000, 0.2468, 0.2336, 0.2456],\n",
      "        [0.7075, 0.2468, 0.0000, 0.2452, 0.2378],\n",
      "        [0.7069, 0.2336, 0.2452, 0.0000, 0.2448],\n",
      "        [0.7030, 0.2456, 0.2378, 0.2448, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n",
      "ew_embs tensor([9.2648, 8.2627, 8.0934, 7.9387, 8.0812], device='cuda:0')\n",
      "tensor([[0.0000, 0.3896, 0.4183, 0.4228, 0.4261],\n",
      "        [0.3896, 0.0000, 0.5871, 0.5819, 0.5686],\n",
      "        [0.4183, 0.5871, 0.0000, 0.6008, 0.6032],\n",
      "        [0.4228, 0.5819, 0.6008, 0.0000, 0.6111],\n",
      "        [0.4261, 0.5686, 0.6032, 0.6111, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SqrtBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 total loss 0.1687417117666996 0.00017706370594616959\n",
      "epoch 51 total loss 0.1641886795111418 0.0001722861275038214\n",
      "epoch 52 total loss 0.1604826781656925 0.00016839735379401102\n",
      "epoch 53 total loss 0.16437056556389962 0.00017247698380262288\n",
      "epoch 54 total loss 0.16477095293407729 0.00017289711745443578\n",
      "aida-A \u001b[92mmicro F1: 0.9180670076192464\u001b[0m\n",
      "aida-B \u001b[92mmicro F1: 0.9297658862876255\u001b[0m\n",
      "msnbc \u001b[92mmicro F1: 0.9410864575363428\u001b[0m\n",
      "aquaint \u001b[92mmicro F1: 0.8811188811188811\u001b[0m\n",
      "ace2004 \u001b[92mmicro F1: 0.8611670020120724\u001b[0m\n",
      "clueweb \u001b[92mmicro F1: 0.7754222062522458\u001b[0m\n",
      "wikipedia \u001b[92mmicro F1: 0.7850011093854005\u001b[0m\n",
      "change learning rate to 1e-05\n",
      "save model to \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\caffe2\\serialize\\inline_container.cc:365] . invalid file name: .state_dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: args\u001b[38;5;241m.\u001b[39mlearning_rate, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: args\u001b[38;5;241m.\u001b[39mn_epochs}\n\u001b[0;32m     51\u001b[0m pprint(config)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mranker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m org_dev_datasets \u001b[38;5;241m=\u001b[39m dev_datasets  \u001b[38;5;66;03m# + [('aida-train', conll.train)]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 350\u001b[0m, in \u001b[0;36mEDRanker.train\u001b[1;34m(self, org_train_dataset, org_dev_datasets, config)\u001b[0m\n\u001b[0;32m    348\u001b[0m         best_f1 \u001b[38;5;241m=\u001b[39m dev_f1\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave model to\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel_path)\n\u001b[1;32m--> 350\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_better_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_not_inc:\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\msc-thesis-workspace\\mulrel-nel\\nel\\abstract_word_entity.py:69\u001b[0m, in \u001b[0;36mAbstractWordEntity.save\u001b[1;34m(self, path, suffix, save_config)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, save_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_config:\n\u001b[0;32m     72\u001b[0m         config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_voca\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_voca\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m,\n\u001b[0;32m     73\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_voca\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentity_voca\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\entity_linking\\lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\caffe2\\serialize\\inline_container.cc:365] . invalid file name: .state_dict"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print('load conll at', datadir)\n",
    "    conll = CoNLLDataset(datadir, person_path, conll_path)\n",
    "\n",
    "    print('create model')\n",
    "    word_voca, word_embeddings = load_voca_embs(voca_emb_dir + 'dict.word',\n",
    "                                                      voca_emb_dir + 'word_embeddings.npy')\n",
    "    print('word voca size', word_voca.size())\n",
    "    snd_word_voca, snd_word_embeddings = load_voca_embs(voca_emb_dir + '/glove/dict.word',\n",
    "                                                              voca_emb_dir + '/glove/word_embeddings.npy')\n",
    "    print('snd word voca size', snd_word_voca.size())\n",
    "\n",
    "    entity_voca, entity_embeddings = load_voca_embs(voca_emb_dir + 'dict.entity',\n",
    "                                                          voca_emb_dir + 'entity_embeddings.npy')\n",
    "    config = {'hid_dims': args.hid_dims,\n",
    "              'emb_dims': entity_embeddings.shape[1],\n",
    "              'freeze_embs': True,\n",
    "              'tok_top_n': args.tok_top_n,\n",
    "              'margin': args.margin,\n",
    "              'word_voca': word_voca,\n",
    "              'entity_voca': entity_voca,\n",
    "              'word_embeddings': word_embeddings,\n",
    "              'entity_embeddings': entity_embeddings,\n",
    "              'snd_word_voca': snd_word_voca,\n",
    "              'snd_word_embeddings': snd_word_embeddings,\n",
    "              'dr': args.dropout_rate,\n",
    "              'args': args}\n",
    "\n",
    "    if ModelClass == MulRelRanker:\n",
    "        config['df'] = args.df\n",
    "        config['n_loops'] = args.n_loops\n",
    "        config['n_rels'] = args.n_rels\n",
    "        config['mulrel_type'] = args.mulrel_type\n",
    "    else:\n",
    "        raise Exception('unknown model class')\n",
    "\n",
    "    pprint(config)\n",
    "    ranker = EDRanker(config=config)\n",
    "\n",
    "    dev_datasets = [('aida-A', conll.testA),\n",
    "                    ('aida-B', conll.testB),\n",
    "                    ('msnbc', conll.msnbc),\n",
    "                    ('aquaint', conll.aquaint),\n",
    "                    ('ace2004', conll.ace2004),\n",
    "                    ('clueweb', conll.clueweb),\n",
    "                    ('wikipedia', conll.wikipedia)\n",
    "                    ]\n",
    "\n",
    "    print('training...')\n",
    "    config = {'lr': args.learning_rate, 'n_epochs': args.n_epochs}\n",
    "    pprint(config)\n",
    "    ranker.train(conll.train, dev_datasets, config)\n",
    "\n",
    "    print(\"\\nEvaluating...\")\n",
    "    org_dev_datasets = dev_datasets  # + [('aida-train', conll.train)]\n",
    "    dev_datasets = []\n",
    "    for dname, data in org_dev_datasets:\n",
    "        dev_datasets.append((dname, ranker.get_data_items(data, predict=True)))\n",
    "        print(dname, '#dev docs', len(dev_datasets[-1][1]))\n",
    "\n",
    "    vecs = ranker.model.rel_embs.cpu().data.numpy()\n",
    "\n",
    "    for di, (dname, data) in enumerate(dev_datasets):\n",
    "        ranker.model._coh_ctx_vecs = []\n",
    "        predictions = ranker.predict(data)\n",
    "        print(dname, tokgreen('micro F1: ' + str(dataset_eval(org_dev_datasets[di][1], predictions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfeb86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c9b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bb06d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entity_linking",
   "language": "python",
   "name": "entity_linking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
